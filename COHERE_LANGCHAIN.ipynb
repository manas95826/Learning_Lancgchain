{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDOqxehOeEhOSnR8A2xt0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manas95826/Learning_Langchain/blob/main/COHERE_LANGCHAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COHERE LANGCHAIN**"
      ],
      "metadata": {
        "id": "7QWUMOafPP9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXNuStFyNXGo",
        "outputId": "5c0e73ab-cb2b-483f-ca46-9bc50689a37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q cohere langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accessing the API requires an API key, which you can get by creating an account and heading here : [ChatCohere API](https://dashboard.cohere.com/api-keys)**"
      ],
      "metadata": {
        "id": "6WfFHC0lO39r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatCohere\n",
        "\n",
        "llm = ChatCohere(cohere_api_key=\"bxRrH9pjWe9TmIiA6dFzmVZcWCADHSHiQcMfwEMj\")"
      ],
      "metadata": {
        "id": "DyH9RUNsNodp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"how can langsmith help with testing?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV8o8mLgNsjG",
        "outputId": "5127dd2a-ec1a-40ba-b613-2e6ac9bc9d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Langsmith, an open-source natural language processing (NLP) toolkit, can assist with various aspects of software testing, enhancing the efficiency and effectiveness of the process. Here's how Langsmith can contribute:\\n\\n1. Test Automation: Langsmith can help create automated tests for language-related features and functionalities. It provides tools and utilities to develop scripts that automate the testing of text processing, natural language understanding, machine translation, and sentiment analysis components. Automated testing saves time, ensures consistency, and allows for frequent and thorough validation of language-based systems.\\n\\n2. Natural Language Generation: Langsmith offers functionalities to generate diverse and realistic text, which can be leveraged to create test data. It can help in generating inputs that cover a wide range of scenarios, including edge cases and corner cases, ensuring comprehensive test coverage. By using Langsmith to generate diverse test inputs, you can validate the behavior of your system across different language situations.\\n\\n3. Text Processing and Analysis: Langsmith's capabilities in text processing, such as tokenization, part-of-speech tagging, named entity recognition, and sentiment analysis, can be employed to preprocess and analyze test data. This aids in ensuring data quality, identifying potential issues in text handling, and verifying the accuracy of text-processing algorithms used in the software under test.\\n\\n4. Language Model Evaluation: Langsmith facilitates the evaluation of language models trained for various tasks, such as question answering, machine translation, or text generation. It provides metrics and analysis tools to assess the performance and accuracy of these models, helping you validate their effectiveness and identify areas needing improvement.\\n\\n5. Localization and Internationalization Testing: Langsmith can assist in localization testing by enabling the testing of software across different languages and locales. It helps ensure that your application handles multiple languages correctly, displays text appropriately, and adheres to local cultural norms.\\n\\n6. Grammar and Syntax Checking: Langsmith can help in developing tests to verify the proper handling of grammar and syntax in text-related operations. It can identify potential issues in language understanding and generation systems, ensuring that the software produces grammatically correct and coherent outputs.\\n\\n7. Collaboration and Documentation: Langsmith fosters collaboration among team members by providing a shared toolkit. It offers documentation and tutorials, enabling a common understanding of testing approaches and methodologies, thereby improving communication and consistency within the testing team.\\n\\n8. Test Data Generation: Langsmith's ability to generate diverse text can be beneficial in creating test datasets of varying lengths and complexities. This helps in evaluating the scalability and performance of language-processing systems and ensures that the software can handle different text volumes and scenarios.\\n\\n9. Sentiment Analysis and User Experience Testing: Langsmith's sentiment analysis capabilities can be leveraged to analyze user feedback and gauge user experiences during testing. It can help identify potential issues that may impact user satisfaction and overall user experience.\\n\\nBy incorporating Langsmith into the testing process, you can enhance the coverage, efficiency, and quality of your software tests, especially for applications that involve natural language processing and understanding. Langsmith's versatile nature and language processing capabilities make it a valuable tool in the testing realm.\", response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'token_count': {'prompt_tokens': 74, 'response_tokens': 629, 'total_tokens': 703, 'billed_tokens': 637}})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are world class technical documentation writer.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "t2CBBIC-Oana"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "QYQiWSWnP9K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDk2yX_eP_ja",
        "outputId": "3ec497f0-59c4-47ef-a096-24059d382450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Langsmith, a natural language processing tool, can assist with testing in numerous ways, offering significant advantages to the process. Here are some ways Langsmith can be instrumental in aiding testing efforts:\\n\\n1. Test Case Generation: Langsmith can help create comprehensive test cases automatically based on specified patterns or templates. It can analyze the application's functionality and generate detailed test cases, ensuring nothing is missed. This capability reduces the time and effort required for manual test case development, especially in complex applications. \\n\\n2. Test Data Preparation: Generating diverse and meaningful test data can be tedious and time-consuming. Langsmith can assist by producing various test datasets, including different input formats, boundary values, and edge cases. It can help ensure that testers have a wide range of data to cover various scenarios, enhancing test coverage and uncovering potential defects effectively. \\n\\n3. Test Script Creation: Langsmith's natural language processing capabilities can transform functional specifications or user stories into test scripts. By understanding the language used to describe application functionality, it can extract key information and create structured test scripts. This automates the test script creation process, saving considerable time and effort. \\n\\n4. Test Execution and Reporting: Langsmith can participate in executing tests and providing detailed reports. It can automate the execution of test cases, allowing testers to focus on analyzing results. Langsmith can also generate reports in a structured format, usually in the form of test logs, test coverage analysis, or summary reports. This streamlines the testing process and makes it more efficient. \\n\\n5. Regression Testing: Langsmith can be incredibly useful in regression testing, ensuring that changes or new features do not introduce unexpected side effects or regressions. By automating the execution of regression test suites and comparing them with previous results, Langsmith helps identify any regressions quickly. \\n\\n6. Exploratory Testing: Langsmith can assist in exploratory testing by providing a structured approach and real-time assistance. It can offer information on the application's functionality, potential risks, or areas that require further investigation. Exploratory testers can leverage Langsmith's insights to navigate the application more effectively and identify potential issues. \\n\\n7. Localization Testing: When dealing with multilingual applications, Langsmith can help with localization testing. It can translate test cases, messages, or user interfaces into different languages, enabling testers to validate the application's behavior across various localizations. \\n\\n8. Integration with Test Management Tools: Langsmith can integrate with popular test management tools, making it possible to manage and track test activities efficiently. It can help create test plans, organize test cases, and link defects to specific tests, facilitating a more organized and manageable testing process. \\n\\nBy harnessing the capabilities of Langsmith in testing, organizations can achieve improved test coverage, efficiency, and effectiveness. It allows testing teams to focus more on analyzing and interpreting results, enhancing the overall quality of the software and reducing the time and effort spent on manual, repetitive tasks.\", response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'token_count': {'prompt_tokens': 85, 'response_tokens': 586, 'total_tokens': 671, 'billed_tokens': 602}})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The output of a ChatModel (and therefore, of this chain) is a message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.***"
      ],
      "metadata": {
        "id": "auw9Y7qhQNhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "uPU2lD0eQEKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "1ovbKk2BQT8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "37ZChQLzQXi-",
        "outputId": "086b446f-36fc-4dcf-916f-23f2c0e4b7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Langsmith, a language-focused development toolkit, offers features that can significantly aid in software testing, enhancing the efficiency and effectiveness of the process. Here's how Langsmith can assist in various testing aspects:\\n\\n1. **Syntax and Semantic Checking**: Langsmith provides robust language processing capabilities, enabling the development of tools that check the syntax and semantics of the code being tested. It can help identify syntax errors, undefined functions, or incorrect usage of language constructs, ensuring the code is structurally sound.\\n\\n2. **Code Linters**: Langsmith enables the creation of code linters, which are tools that analyze code quality and flag potential issues or deviations from a defined coding style. This can assist in identifying formatting inconsistencies, unused variables, or potential logical errors, making code easier to maintain and debug.\\n\\n3. **Unit Testing**: Langsmith simplifies the creation of unit tests, allowing developers to write concise and expressive tests for individual units of code. It helps ensure that specific functions or methods behave as expected, contributing to the modular testing of the software.\\n\\n4. **Mocking and Stubbing**: Langsmith facilitates the generation of mock objects and stub functions, which are crucial for isolating the code being tested from its dependencies. By creating mock versions of external components, developers can focus solely on testing the target code's functionality, thus enhancing test reliability.\\n\\n5. **Code Coverage Analysis**: Langsmith can aid in code coverage analysis, helping developers measure the extent to which their tests cover the codebase. It identifies untested or poorly tested sections of code, guiding developers to focus their testing efforts effectively and ensure comprehensive coverage.\\n\\n6. **Regression Testing**: Langsmith supports the development of tools for regression testing, allowing developers to re-run tests on existing code after changes or updates. This helps catch regressions and ensures that previous functionalities remain intact, preventing the introduction of new bugs.\\n\\n7. **Natural Language Assertions**: Langsmith's language processing capabilities enable the formulation of assertions in natural language. Instead of using cryptic regular expressions, developers can express expectations in plain text, making tests more readable and maintainable.\\n\\n8. **Test Reporting and Documentation**: Langsmith assists in generating comprehensive test reports and documentation. It can help create visually appealing and informative reports, summarizing test results, providing code coverage visuals, and detailing any encountered issues.\\n\\n9. **Integration with Testing Frameworks**: Langsmith integrates well with popular testing frameworks, such as Jest, Mocha, or Cypress. This integration simplifies the implementation of Langsmith-powered testing utilities within existing testing workflows.\\n\\n10. **Language-Specific Testing**: Langsmith's language-centric approach allows developers to write testing utilities tailored to the specific language they are testing. This enables the creation of language-specific assertions, helpers, and matchers, resulting in more accurate and relevant testing.\\n\\nBy leveraging Langsmith's capabilities, developers can establish a robust testing environment, streamline their testing workflows, and improve the overall quality of their software projects. Langsmith simplifies the creation of sophisticated testing tools and enhances the clarity and precision of tests, leading to more reliable and maintainable codebases.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZCSovJ-QX3x",
        "outputId": "ae790cf5-7e56-41bf-893e-63bd9cdc9847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "e2S4HRXtSkxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "\n",
        "embeddings = CohereEmbeddings(cohere_api_key=\"bxRrH9pjWe9TmIiA6dFzmVZcWCADHSHiQcMfwEMj\")"
      ],
      "metadata": {
        "id": "5IU9gJYGSmo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DZyfkjOSsy2",
        "outputId": "a3c9480f-7e87-4ded-ad9c-51d1d7254884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "documents = text_splitter.split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "cVwSBjiSS7m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)"
      ],
      "metadata": {
        "id": "BuFRy9GSS_gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_chain.invoke({\n",
        "    \"input\": \"how can langsmith help with testing?\",\n",
        "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DzRHkRlWTMRy",
        "outputId": "a1d9d625-e2f1-402f-dab1-5d526e2f2c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Langsmith can help with testing by enabling users to visualize test results.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retriever = vector.as_retriever()\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ],
      "metadata": {
        "id": "_Rk3U-tvTSZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
        "print(response[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A4c8ZJfTWNx",
        "outputId": "fbc132f0-6fe0-4bf2-a37a-c731144ef11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, that can be used to run tests on LLM applications. Test cases can be created on the fly, uploaded in bulk, or exported from application traces. Custom evaluations can also be run to score the test results. \n",
            "\n",
            "LangSmith also offers a comparison view to assess different versions of an application. This enables developers to determine if recent changes have led to any regressions by viewing the test results of different configurations side-by-side. \n",
            "\n",
            "The platform further assists in beta testing by helping developers comprehend the performance of their applications in real-world scenarios. Feedback collection and run annotation are vital to this process, aiding in the curation of test cases and the identification of improvements. Users can attach feedback scores to logged traces, which can then be filtered based on specific criteria. Additionally, runs can be sent to annotation queues for detailed inspection by annotators, which includes evaluating them against different criteria.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "# First we need a prompt that we can pass into an LLM to generate this search query\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
        "])\n",
        "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
      ],
      "metadata": {
        "id": "G6qMN4ZTTZ01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
        "retriever_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8UdIQyWTnfj",
        "outputId": "a4c8c6d6-3292-45f0-ba4d-8caf40f84d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.Prototyping‚ÄãPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.Debugging‚ÄãWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set‚ÄãWhile many developers still ship an initial version of their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View‚ÄãWhen prototyping different versions of your applications and making changes, it‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground‚ÄãLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
              " Document(page_content=\"This allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.\\nEvery playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing‚ÄãBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback‚ÄãWhen launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responses it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces‚ÄãLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset‚ÄãAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production‚ÄãClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production. However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Monitoring and A/B Testing‚ÄãLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Was this page helpful?PreviousLangSmithNextSetupPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
              " Document(page_content='LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'})]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
      ],
      "metadata": {
        "id": "Br2NDOETTptO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
        "retrieval_chain.invoke({\n",
        "    \"chat_history\": chat_history,\n",
        "    \"input\": \"Tell me how\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alHHITLoTyeK",
        "outputId": "2b7d69a9-e375-46af-e77d-6fe4859166fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
              "  AIMessage(content='Yes!')],\n",
              " 'input': 'Tell me how',\n",
              " 'context': [Document(page_content='Skip to main content\\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith DocsLangChain Python DocsLangChain JS/TS DocsLangSmith API DocsSearchGo to AppLangSmithUser GuideSetupPricing (Coming Soon)Self-HostingTracingEvaluationMonitoringPrompt HubProxyCookbookUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.Prototyping‚ÄãPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.Debugging‚ÄãWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set‚ÄãWhile many developers still ship an initial version of their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View‚ÄãWhen prototyping different versions of your applications and making changes, it‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground‚ÄãLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
              "  Document(page_content=\"This allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.\\nEvery playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing‚ÄãBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback‚ÄãWhen launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responses it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces‚ÄãLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset‚ÄãAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production‚ÄãClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production. However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Monitoring and A/B Testing‚ÄãLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Was this page helpful?PreviousLangSmithNextSetupPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright ¬© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
              "  Document(page_content='LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'})],\n",
              " 'answer': \"LangSmith is a very useful platform for testing and evaluating LLM applications. Here's how it can help:\\n1. **Creating Test Datasets**: LangSmith allows developers to create datasets, which are essentially collections of input queries and their corresponding reference outputs. These datasets can be used to evaluate the performance of LLM applications, by comparing their actual outputs with the reference outputs. Datasets can be uploaded in bulk, created on the fly, or exported from application traces.\\n2. **Running Tests**: You can use LangSmith to run tests on your LLM applications using the created datasets. It provides an efficient way to validate your application's behavior and identify any deviations or errors.\\n3. **Custom Evaluations**: LangSmith also enables you to conduct custom evaluations, which can be both LLM-based or heuristic-based, to score the test results. This helps in assessing the performance of your application using different evaluation metrics.\\n4. **Comparison View**: LangSmith offers a comparison view feature that allows you to compare the results of different application variants side-by-side. This is extremely useful when prototyping different versions of your application to understand the impact of changes. By comparing test scores across various revisions, you can easily track improvements or regressions.\\n5. **Playground Environment**: The platform provides a playground environment for quick experimentation. You can test different prompts and models, and every playground run is logged in the system for further analysis or creating test cases.\\n6. **Beta Testing**: LangSmith assists in the beta testing phase by enabling feedback collection and run annotation. You can collect user feedback on the application's responses and annotate logged traces to analyze and improve performance. \\n7. **Monitoring and A/B Testing**: In the production phase, LangSmith's monitoring charts provide a high-level overview of your application's performance. You can track metrics like latency, cost, and feedback scores over time. Additionally, LangSmith supports A/B testing, allowing you to compare different versions of your application using tags or metadata.\\n\\nOverall, LangSmith offers a comprehensive suite of tools that cover the entire development lifecycle of LLM applications, including prototype, debugging, testing, beta testing, and production phases.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Agent**"
      ],
      "metadata": {
        "id": "0wxvf0zcUEW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"langsmith_search\",\n",
        "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
        ")"
      ],
      "metadata": {
        "id": "ArlWLKSeT0YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "cAp7Q6EpVxmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "search = TavilySearchResults()"
      ],
      "metadata": {
        "id": "C_DPn9-aUOKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [retriever_tool, search]"
      ],
      "metadata": {
        "id": "r4cmM4hoWOu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchainhub langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs90dLDQWRKU",
        "outputId": "689e866d-e2db-485d-d6ba-9a88a859576e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "l9UiC6wIWiA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "\n",
        "# Get the prompt to use - you can modify this!\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
      ],
      "metadata": {
        "id": "zWZB-cupWXDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web Search with Cohere**"
      ],
      "metadata": {
        "id": "Iiluv8_vUh8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q cohere"
      ],
      "metadata": {
        "id": "IE8gxd2vUrWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62e1077-e00b-4716-8189-1b6b06127f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/141.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m133.1/141.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.6/141.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "co = cohere.Client('bxRrH9pjWe9TmIiA6dFzmVZcWCADHSHiQcMfwEMj')\n",
        "response = co.chat(\n",
        "  chat_history=[\n",
        "    {\"role\": \"USER\", \"message\": \"What is langchain?\"},\n",
        "    {\"role\": \"CHATBOT\", \"message\": \"LangChain is a framework for developing applications powered by language models.\"}\n",
        "  ],\n",
        "  message=\"What is langserve and langsmith and how it's different from langchain?\",\n",
        "  # perform web search before answering the question. You can also use your own custom connector.\n",
        "  connectors=[{\"id\": \"web-search\"}]\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQsNd8BYG9bY",
        "outputId": "f7795e19-7c05-433c-be01-0d2f39d08696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='LangChain is an open-source framework for building applications using Large Language Models (LLMs). The framework simplifies the creation of applications by providing abstractions and tools to improve the customization, accuracy, and relevance of the information generated by the models. \\n\\nLangServe is a library for deploying LangChain applications as a REST API. It makes it easy to deploy any LangChain chain/agent/runnable and is designed for developers who leverage AI in their applications. LangServe provides features like automatic input and output schema inference, efficient API endpoints, and built-in monitoring through LangSmith.\\n\\nLangSmith is a platform for debugging, testing, evaluating, and monitoring LLM applications. It offers a unified developer platform for building, testing, and monitoring LLM applications built with or without LangChain. LangSmith helps in tracing and evaluating applications and provides a range of tutorials and real-world examples to assist developers.' generation_id='4487a9d7-e488-475c-b64e-f49420bb8631' citations=[ChatCitation(start=16, end=27, text='open-source', document_ids=['web-search_2', 'web-search_3', 'web-search_5']), ChatCitation(start=28, end=37, text='framework', document_ids=['web-search_1', 'web-search_2', 'web-search_3', 'web-search_5', 'web-search_9']), ChatCitation(start=42, end=91, text='building applications using Large Language Models', document_ids=['web-search_1', 'web-search_2', 'web-search_3', 'web-search_4', 'web-search_5', 'web-search_9']), ChatCitation(start=92, end=98, text='(LLMs)', document_ids=['web-search_1', 'web-search_2', 'web-search_3', 'web-search_4', 'web-search_5']), ChatCitation(start=114, end=153, text='simplifies the creation of applications', document_ids=['web-search_2', 'web-search_3', 'web-search_5']), ChatCitation(start=167, end=179, text='abstractions', document_ids=['web-search_3', 'web-search_9']), ChatCitation(start=184, end=189, text='tools', document_ids=['web-search_3', 'web-search_9']), ChatCitation(start=193, end=218, text='improve the customization', document_ids=['web-search_3']), ChatCitation(start=220, end=228, text='accuracy', document_ids=['web-search_3']), ChatCitation(start=234, end=243, text='relevance', document_ids=['web-search_3']), ChatCitation(start=251, end=287, text='information generated by the models.', document_ids=['web-search_3']), ChatCitation(start=305, end=312, text='library', document_ids=['web-search_12', 'web-search_14']), ChatCitation(start=317, end=349, text='deploying LangChain applications', document_ids=['web-search_0', 'web-search_9', 'web-search_12', 'web-search_13', 'web-search_14', 'web-search_15']), ChatCitation(start=355, end=364, text='REST API.', document_ids=['web-search_12', 'web-search_14']), ChatCitation(start=377, end=426, text='easy to deploy any LangChain chain/agent/runnable', document_ids=['web-search_0', 'web-search_13', 'web-search_15']), ChatCitation(start=434, end=457, text='designed for developers', document_ids=['web-search_14']), ChatCitation(start=462, end=496, text='leverage AI in their applications.', document_ids=['web-search_14']), ChatCitation(start=530, end=573, text='automatic input and output schema inference', document_ids=['web-search_12', 'web-search_13', 'web-search_15']), ChatCitation(start=575, end=598, text='efficient API endpoints', document_ids=['web-search_12', 'web-search_14', 'web-search_15']), ChatCitation(start=604, end=642, text='built-in monitoring through LangSmith.', document_ids=['web-search_12', 'web-search_14', 'web-search_15']), ChatCitation(start=659, end=667, text='platform', document_ids=['web-search_0', 'web-search_10', 'web-search_11', 'web-search_18']), ChatCitation(start=672, end=681, text='debugging', document_ids=['web-search_11', 'web-search_18']), ChatCitation(start=683, end=690, text='testing', document_ids=['web-search_10', 'web-search_11', 'web-search_18']), ChatCitation(start=692, end=702, text='evaluating', document_ids=['web-search_10', 'web-search_11', 'web-search_18']), ChatCitation(start=708, end=736, text='monitoring LLM applications.', document_ids=['web-search_0', 'web-search_10', 'web-search_11', 'web-search_18']), ChatCitation(start=749, end=775, text='unified developer platform', document_ids=['web-search_0', 'web-search_1', 'web-search_4', 'web-search_9']), ChatCitation(start=780, end=788, text='building', document_ids=['web-search_0', 'web-search_1', 'web-search_4']), ChatCitation(start=790, end=797, text='testing', document_ids=['web-search_0', 'web-search_1', 'web-search_4', 'web-search_11', 'web-search_18']), ChatCitation(start=803, end=830, text='monitoring LLM applications', document_ids=['web-search_0', 'web-search_1', 'web-search_4']), ChatCitation(start=831, end=863, text='built with or without LangChain.', document_ids=['web-search_0', 'web-search_1']), ChatCitation(start=883, end=890, text='tracing', document_ids=['web-search_10', 'web-search_11', 'web-search_18']), ChatCitation(start=895, end=918, text='evaluating applications', document_ids=['web-search_11', 'web-search_18']), ChatCitation(start=934, end=952, text='range of tutorials', document_ids=['web-search_10', 'web-search_11', 'web-search_18']), ChatCitation(start=957, end=976, text='real-world examples', document_ids=['web-search_10', 'web-search_11', 'web-search_17', 'web-search_18'])] documents=[{'id': 'web-search_2', 'snippet': 'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project\\'s Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\\n\\nIn October 2023 LangChain introduced LangServe, a deployment tool designed to facilitate the transition from LCEL (LangChain Expression Language) prototypes to production-ready applications.\\n\\nLangChain\\'s developers highlight the framework\\'s applicability to use-cases including chatbots, retrieval-augmented generation, document summarization, and synthetic data generation.\\n\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database to store and retrieve vector embeddings; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\\n\\nProvides financial market data and analytics\\n\\nFinancial data, analytics\\n\\nhttps://python.langchain.com/docs/integrations/tools/alpha_vantage\\n\\nWeb scraping and automation platform\\n\\nWeb scraping, automation\\n\\nhttps://python.langchain.com/docs/integrations/tools/apify\\n\\nAccess to scientific papers and research\\n\\nScientific papers, research\\n\\nhttps://python.langchain.com/docs/integrations/tools/arxiv\\n\\nServerless computing service\\n\\nServerless computing\\n\\nhttps://python.langchain.com/docs/integrations/tools/awslambda\\n\\nAccess to the shell environment\\n\\nShell environment access\\n\\nhttps://python.langchain.com/docs/integrations/tools/bash\\n\\nBearly Code Interpreter\\n\\nRemote execution of Python code\\n\\nPython code execution\\n\\nhttps://python.langchain.com/docs/integrations/tools/bearly\\n\\nSearch engine powered by Microsoft Bing\\n\\nhttps://python.langchain.com/docs/integrations/tools/bing_search\\n\\nPrivacy-focused search engine\\n\\nPrivacy-focused search\\n\\nhttps://python.langchain.com/docs/integrations/tools/brave_search\\n\\nPlugins for ChatGPT language model\\n\\nhttps://python.langchain.com/docs/integrations/tools/chatgpt_plugins\\n\\nAction Tool Tool for performing actions using the Connery API\\n\\nhttps://python.langchain.com/docs/integrations/tools/connery\\n\\nDall-E Image Generator\\n\\nText-to-image generation using OpenAI\\'s DALL-E model\\n\\nText-to-image generation\\n\\nhttps://python.langchain.com/docs/integrations/tools/dalle_image_generator\\n\\nSEO data and analytics platform\\n\\nhttps://python.langchain.com/docs/integrations/tools/dataforseo\\n\\nPrivacy-focused search engine\\n\\nhttps://python.langchain.com/docs/integrations/tools/ddg\\n\\nSandbox environment for running Python code for data analysis\\n\\nData analysis environment\\n\\nhttps://python.langchain.com/docs/integrations/tools/e2b_data_analysis\\n\\nSuite of AI tools and APIs\\n\\nhttps://python.langchain.com/docs/integrations/tools/edenai_tools\\n\\nEleven Labs Text2Speech\\n\\nText-to-speech API by Eleven Labs\\n\\nhttps://python.langchain.com/docs/integrations/tools/eleven_labs_tts\\n\\nSearch engine access\\n\\nhttps://python.langchain.com/docs/integrations/tools/exa_search\\n\\nTools for interacting with the local file system\\n\\nFile system interaction\\n\\nhttps://python.langchain.com/docs/integrations/tools/filesystem\\n\\nNatural language APIs for querying various services\\n\\nNatural language queries\\n\\nhttps://python.langchain.com/docs/integrations/tools/golden_query\\n\\nGoogle Cloud Text-to-Speech\\n\\nText-to-speech API by Google Cloud\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_cloud_texttospeech\\n\\nAccess and manage files on Google Drive\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_drive\\n\\nAccess financial data from Google Finance\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_finance\\n\\nSearch for job listings using Google Jobs API\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_jobs\\n\\nVisual search and recognition tool by Google\\n\\nVisual search, recognition\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_lens\\n\\nAccess to Google Places API for location-based services\\n\\nLocation-based services\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_places\\n\\nSearch for scholarly articles using Google Scholar API\\n\\nScholarly article search\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_scholar\\n\\nSearch engine powered by Google\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_search\\n\\nSearch engine results page (SERP) scraping tool\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_serper\\n\\nAccess to Google Trends data\\n\\nhttps://python.langchain.com/docs/integrations/tools/google_trends\\n\\nLibrary for creating UIs for machine learning models\\n\\nMachine learning UIs\\n\\nhttps://python.langchain.com/docs/integrations/tools/gradio_tools\\n\\nQuery language for APIs\\n\\nhttps://python.langchain.com/docs/integrations/tools/graphql\\n\\nTools for working with Hugging Face models and datasets\\n\\nHugging Face models, datasets\\n\\nhttps://python.langchain.com/docs/integrations/tools/huggingface_tools\\n\\nUse human input as a tool for AI\\n\\nhttps://python.langchain.com/docs/integrations/tools/human_tools\\n\\nConnect and automate various web services\\n\\nWeb service automation\\n\\nhttps://python.langchain.com/docs/integrations/tools/ifttt\\n\\nTool for shopping using the Ionic API\\n\\nhttps://python.langchain.com/docs/integrations/tools/ionic_shopping\\n\\nTool for interacting with the Lemon AI platform\\n\\nLemon AI interaction\\n\\nhttps://python.langchain.com/docs/integrations/tools/lemonai\\n\\nTool for memorizing information using unsupervised learning\\n\\nhttps://python.langchain.com/docs/integrations/tools/memorize\\n\\nUnderstanding Tool for indexing unstructured data using Nuclia\\n\\nhttps://python.langchain.com/docs/integrations/tools/nuclia\\n\\nAccess to weather data using OpenWeatherMap API\\n\\nhttps://python.langchain.com/docs/integrations/tools/openweathermap\\n\\nPolygon Stock Market API\\n\\nAccess to stock market data using Polygon API\\n\\nhttps://python.langchain.com/docs/integrations/tools/polygon\\n\\nAccess to biomedical literature using PubMed API\\n\\nBiomedical literature\\n\\nhttps://python.langchain.com/docs/integrations/tools/pubmed\\n\\nInteractive Python shell\\n\\nhttps://python.langchain.com/docs/integrations/tools/python\\n\\nSearch for content on Reddit\\n\\nhttps://python.langchain.com/docs/integrations/tools/reddit_search\\n\\nHTTP library for making requests\\n\\nhttps://python.langchain.com/docs/integrations/tools/requests\\n\\nTool for explaining the predictions of machine learning models\\n\\nhttps://python.langchain.com/docs/integrations/tools/sceneXplain\\n\\nCollection of tools for searching and querying various services\\n\\nhttps://python.langchain.com/docs/integrations/tools/search_tools\\n\\nTool for searching and querying various APIs\\n\\nhttps://python.langchain.com/docs/integrations/tools/searchapi\\n\\nSearch Privacy-focused metasearch engine\\n\\nPrivacy-focused search\\n\\nhttps://python.langchain.com/docs/integrations/tools/searx_search\\n\\nSemantic Scholar API\\n\\ntool Access to academic papers using the Semantic Scholar API\\n\\nAcademic paper search\\n\\nhttps://python.langchain.com/docs/integrations/tools/semanticscholar\\n\\nSearch engine results page (SERP) scraping tool\\n\\nhttps://python.langchain.com/docs/integrations/tools/serpapi\\n\\nAccess to the Stack Exchange network\\n\\nStack Exchange access\\n\\nhttps://python.langchain.com/docs/integrations/tools/stackexchange\\n\\nSearch engine for finding answers to questions\\n\\nhttps://python.langchain.com/docs/integrations/tools/tavily_search\\n\\nCommunication APIs for SMS, voice, and video\\n\\nhttps://python.langchain.com/docs/integrations/tools/twilio\\n\\nAccess to structured data from Wikidata\\n\\nStructured data access\\n\\nhttps://python.langchain.com/docs/integrations/tools/wikidata\\n\\nAccess to articles and information from Wikipedia\\n\\nhttps://python.langchain.com/docs/integrations/tools/wikipedia\\n\\nComputational knowledge engine\\n\\nComputational knowledge\\n\\nhttps://python.langchain.com/docs/integrations/tools/wolfram_alpha\\n\\nAccess to financial news using Yahoo Finance API\\n\\nhttps://python.langchain.com/docs/integrations/tools/yahoo_finance_news\\n\\nAccess to YouTube data and functionality\\n\\nhttps://python.langchain.com/docs/integrations/tools/youtube\\n\\nZapier Natural Language Actions\\n\\nIntegration platform for automating workflows\\n\\nhttps://python.langchain.com/docs/integrations/tools/zapier', 'timestamp': '2024-03-22T02:23:04', 'title': 'LangChain - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/LangChain'}, {'id': 'web-search_3', 'snippet': \"Skip to main content\\n\\nClick here to return to Amazon Web Services homepage\\n\\nSign out of AWS Builder ID\\n\\nAWS Management Console\\n\\nBilling & Cost Management\\n\\nSecurity Credentials\\n\\nAWS Personal Health Dashboard\\n\\nAWS Support Overview\\n\\nClick here to return to Amazon Web Services homepage\\n\\nGet Started for Free\\n\\nWhat is Cloud Computing?\\n\\nCloud Computing Concepts Hub\\n\\nMachine Learning & AI\\n\\nCreate an AWS Account\\n\\nExplore Free Machine Learning Offers\\n\\nBuild, deploy, and run machine learning applications in the cloud for free\\n\\nCheck out Machine Learning Services\\n\\nInnovate faster with the most comprehensive set of AI and ML services\\n\\nBrowse Machine Learning Trainings\\n\\nGet started on machine learning training with content built by AWS experts\\n\\nRead Machine Learning Blogs\\n\\nRead about the latest AWS Machine Learning product news and best practices\\n\\nWhat is LangChain? Why is LangChain important? How does LangChain work? What are the core components of LangChain? How can AWS help with your LangChain requirements? \\n\\nLangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queries—for example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain also includes components that allow LLMs to access new data sets without retraining.\\n\\nWhy is LangChain important?\\n\\nLLMs excel at responding to prompts in a general context, but struggle in a specific domain they were never trained on. Prompts are queries people use to seek responses from an LLM. For example, an LLM can provide an answer to how much a computer costs by providing an estimate. However, it can't list the price of a specific computer model that your company sells. \\n\\nTo do that, machine learning engineers must integrate the LLM with the organization’s internal data sources and apply prompt engineering—a practice where a data scientist refines inputs to a generative model with a specific structure and context. \\n\\nLangChain streamlines intermediate steps to develop such data-responsive applications, making prompt engineering more efficient. It is designed to develop diverse applications powered by language models more effortlessly, including chatbots, question-answering, content generation, summarizers, and more.\\n\\nThe following sections describe benefits of LangChain.\\n\\nRepurpose language models\\n\\nWith LangChain, organizations can repurpose LLMs for domain-specific applications without retraining or fine-tuning. Development teams can build complex applications referencing proprietary information to augment model responses. For example, you can use LangChain to build applications that read data from stored internal documents and summarize them into conversational responses. You can create a Retrieval Augmented Generation (RAG) workflow that introduces new information to the language model during prompting. Implementing context-aware workflows like RAG reduces model hallucination and improves response accuracy. \\n\\nSimplify AI development\\n\\nLangChain simplifies artificial intelligence (AI) development by abstracting the complexity of data source integrations and prompt refining. Developers can customize sequences to build complex applications quickly. Instead of programming business logic, software teams can modify templates and libraries that LangChain provides to reduce development time. \\n\\nLangChain provides AI developers with tools to connect language models with external data sources. It is open-source and supported by an active community. Organizations can use LangChain for free and receive support from other developers proficient in the framework.\\n\\nHow does LangChain work?\\n\\nWith LangChain, developers can adapt a language model flexibly to specific business contexts by designating steps required to produce the desired outcome. \\n\\nChains are the fundamental principle that holds various AI components in LangChain to provide context-aware responses. A chain is a series of automated actions from the user's query to the model's output. For example, developers can use a chain for:\\n\\nConnecting to different data sources.\\n\\nGenerating unique content.\\n\\nTranslating multiple languages.\\n\\nAnswering user queries. \\n\\nChains are made of links. Each action that developers string together to form a chained sequence is called a link. With links, developers can divide complex tasks into multiple, smaller tasks. Examples of links include:\\n\\nFormatting user input. \\n\\nSending a query to an LLM. \\n\\nRetrieving data from cloud storage.\\n\\nTranslating from one language to another.\\n\\nIn the LangChain framework, a link accepts input from the user and passes it to the LangChain libraries for processing. LangChain also allows link reordering to create different AI workflows. \\n\\nTo use LangChain, developers install the framework in Python with the following command:\\n\\npip install langchain \\n\\nDevelopers then use the chain building blocks or LangChain Expression Language (LCEL) to compose chains with simple programming commands. The chain() function passes a link's arguments to the libraries. The execute() command retrieves the results. Developers can pass the current link result to the following link or return it as the final output. \\n\\nBelow is an example of a chatbot chain function that returns product details in multiple languages.\\n\\nretrieve_data_from_product_database().\\n\\nsend_data_to_language_model().\\n\\n format_output_in_a_list().\\n\\n translate_output_in_target_language()\\n\\nWhat are the core components of LangChain?\\n\\nUsing LangChain, software teams can build context-aware language model systems with the following modules. \\n\\nLangChain provides APIs with which developers can connect and query LLMs from their code. Developers can interface with public and proprietary models like GPT, Bard, and PaLM with LangChain by making simple API calls instead of writing complex code.\\n\\nPrompt templates are pre-built structures developers use to consistently and precisely format queries for AI models. Developers can create a prompt template for chatbot applications, few-shot learning, or deliver specific instructions to the language models. Moreover, they can reuse the templates across different applications and language models. \\n\\nDevelopers use tools and libraries that LangChain provides to compose and customize existing chains for complex applications. An agent is a special chain that prompts the language model to decide the best sequence in response to a query. When using an agent, developers provide the user's input, available tools, and possible intermediate steps to achieve the desired results. Then, the language model returns a viable sequence of actions the application can take. \\n\\nLangChain enables the architecting of RAG systems with numerous tools to transform, store, search, and retrieve information that refine language model responses. Developers can create semantic representations of information with word embeddings and store them in local or cloud vector databases. \\n\\nSome conversational language model applications refine their responses with information recalled from past interactions. LangChain allows developers to include memory capabilities in their systems. It supports:\\n\\nSimple memory systems that recall the most recent conversations. \\n\\nComplex memory structures that analyze historical messages to return the most relevant results. \\n\\nCallbacks are codes that developers place in their applications to log, monitor, and stream specific events in LangChain operations. For example, developers can track when a chain was first called and errors encountered with callbacks. \\n\\nHow can AWS help with your LangChain requirements? \\n\\nUsing Amazon Bedrock, Amazon Kendra, Amazon SageMaker JumpStart, LangChain, and your LLMs, you can build highly-accurate generative artificial intelligence (generative AI) applications on enterprise data. LangChain is the interface that ties these components together:\\n\\nAmazon Bedrock is a managed service with which organizations can build and deploy generative AI applications. You can use Amazon Bedrock to set up a generational model, which you access from LangChain. \\n\\nAmazon Kendra is a machine learning (ML)-powered service that helps organizations perform internal searches. You can connect Amazon Kendra to LangChain, which uses data from proprietary databases to refine language model outputs. \\n\\nAmazon SageMaker Jumpstart is an ML hub that provides pre-built algorithms and foundational models that developers can deploy quickly. You can host foundational models on SageMaker Jumpstart and prompt them from LangChain. \\n\\nGet started with LangChain on AWS by creating an account today.\\n\\nCheck out additional product-related resources Innovate faster with the most comprehensive set of AI and ML services \\n\\nSign up for a free account\\n\\nInstant get access to the AWS Free Tier. Sign up \\n\\nStart building in the console\\n\\nGet started building in the AWS management console. Sign in \\n\\nEnding Support for Internet Explorer Got it\\n\\nAWS support for Internet Explorer ends on 07/31/2022. Supported browsers are Chrome, Firefox, Edge, and Safari. Learn more »\", 'timestamp': '2024-03-15T17:08:21', 'title': 'What is LangChain? - LangChain Explained - AWS', 'url': 'https://aws.amazon.com/what-is/langchain/'}, {'id': 'web-search_5', 'snippet': \"Use LangChain with watsonx.ai\\n\\nSubscribe for AI updates\\n\\nLangChain is an open source orchestration framework for the development of applications using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and virtual agents.\\n\\nLangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.\\n\\nLaunched by Harrison Chase in October 2022, LangChain enjoyed a meteoric rise to prominence: as of June 2023, it was the single fastest-growing open source project on Github.1 Coinciding with the momentous launch of OpenAI’s ChatGPT the following month, LangChain has played a significant role in making generative AI more accessible to enthusiasts in the wake of its widespread popularity.\\n\\nLangChain can facilitate most use cases for LLMs and natural language processing (NLP), like chatbots, intelligent search, question-answering, summarization services or even virtual agents capable of robotic process automation.\\n\\nIntegrations with LLMs\\n\\nLLMs are not standalone applications: they are pre-trained statistical models that must be paired with an application (and, in some cases, specific data sources) in order to meet their purpose.\\n\\nFor example, Chat-GPT is not an LLM: it is a chatbot application that, depending on the version you’ve chosen, uses the GPT-3.5 or GPT-4 language model. While it’s the GPT model that interprets the user’s input and composes a natural language response, it’s the application that (among other things) provides an interface for the user to type and read and a UX design that governs the chatbot experience. Even at the enterprise level, Chat-GPT is not the only application using the GPT model: Microsoft uses GPT-4 to power Bing Chat.\\n\\nFurthermore, though foundation models (like those powering LLMs) are pre-trained on massive datasets, they are not omniscient. If a particular task requires access to specific contextual information, like internal documentation or domain expertise, LLMs must be connected to those external data sources. Even if you simply want your model to reflect real-time awareness of current events, it requires external information: a model’s internal data is only up-to-date through the time period during which it was pre-trained.\\n\\nLikewise, if a given generative AI task requires access to external software workflows—for example, if you wanted your virtual agent to integrate with Slack—then you will need a way to integrate the LLM with the API for that software.\\n\\nWhile these integrations can generally be achieved with fully manual code, orchestration frameworks like LangChain and the IBM watsonx platform greatly simplify the process. They also make it much easier to experiment with different LLMs to compare results, as different models can be swapped in and out with minimal changes to code.\\n\\nIBM named a leader by Gartner\\n\\nRead why IBM was named a leader in the 2023 Gartner® Magic Quadrant™ for Cloud AI Developer Services report.\\n\\nRegister for the guide on foundation models\\n\\nHow does LangChain work?\\n\\nAt LangChain’s core is a development environment that streamlines the programming of LLM applications through the use of abstraction: the simplification of code by representing one or more complex processes as a named component that encapsulates all of its constituent steps.\\n\\nAbstractions are a common element of everyday life and language. For example, “π” allows us to represent the ratio of the length of a circle’s circumference to that of its diameter without having to write out its infinite digits. Similarly, a thermostat allows us to control the temperature in our home without needing to understand the complex circuitry this entails—we only need to know how different thermostat settings translate to different temperatures.\\n\\nLangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.\\n\\nImporting language models\\n\\nNearly any LLM can be used in LangChain. Importing language models into LangChain is easy, provided you have an API key. The LLM class is designed to provide a standard interface for all models.\\n\\nMost LLM providers will require you to create an account in order to receive an API key. Some of these APIs—particularly those for proprietary closed-source models, like those offered by OpenAI or Anthropic—may have associated costs.\\n\\nMany open source models, like BigScience’s BLOOM, Meta AI’s LLaMa and Google’s Flan-T5, can be accessed through Hugging Face (link resides outside ibm.com). IBM watsonx, through its partnership with Hugging Face, also offers a curated suite of open source models. Creating an account with either service will allow you to generate an API key for any of the models offered by that provider.\\n\\nLangChain is not limited to out-of-the-box foundation models: the CustomLLM class (link resides outside ibm.com) allows for custom LLM wrappers. Likewise, you can use the IBM watsonx APIs and Python SDK, which includes a LangChain integration, to build applications in LangChain with models that you’ve already trained or fine-tuned for your specific needs using the WatsonxLLM class (and that model’s specific project ID).\\n\\nExplore the demo: using watsonx and LangChain to make a series of calls to a language model\\n\\nPrompts are the instructions given to an LLM. The “art” of composing prompts that effectively provide the context necessary for the LLM to interpret input and structure output in the way most useful to you is often called prompt engineering.\\n\\nThe PromptTemplate class in LangChain formalizes the composition of prompts without the need to manually hard code context and queries. Important elements of a prompt are likewise entered as formal classes, like input_variables. A prompt template can thus contain and reproduce context, instructions (like “do not use technical terms”), a set of examples to guide its responses (in what is called “few-shot prompting”), a specified output format or a standardized question to be answered. You can save and name an effectively structured prompt template and easily reuse it as needed.\\n\\nThough these elements can all be manually coded, PromptTemplate modules empower smooth integration with other LangChain features, like the eponymous chains.\\n\\nWatch the video: prompt engineering and prompt tuning\\n\\nAs its name implies, chains are the core of LangChain’s workflows. They combine LLMs with other components, creating applications by executing a sequence of functions.\\n\\nThe most basic chain is LLMChain. It simply calls a model and prompt template for that model. For example, imagine you saved a prompt as “ExamplePrompt” and wanted to run it against Flan-T5. You can import LLMChain from langchain.chains, then define chain_example = LLMChain(llm = flan-t5, prompt = ExamplePrompt). To run the chain for a given input, you simply call chain_example.run(“input”).\\n\\nTo use the output of one function as the input for the next function, you can use SimpleSequentialChain. Each function could utilize different prompts, different tools, different parameters or even different models, depending on your specific needs.\\n\\nTo achieve certain tasks, LLMs will need access to specific external data sources not included in its training dataset, such as internal documents, emails or datasets. LangChain collectively refers to such external documentation as “indexes”.\\n\\nDocument loaders LangChain offers a wide variety of document loaders for third party applications (link resides outside ibm.com). This allows for easy importation of data from sources like file storage services (like Dropbox, Google Drive and Microsoft OneDrive), web content (like YouTube, PubMed or specific URLs), collaboration tools (like Airtable, Trello, Figma and Notion), databases (like Pandas, MongoDB and Microsoft), among many others.\\n\\nVector databases Unlike “traditional” structured databases, vector databases represent data points by converting them into vector embeddings: numerical representations in the form of vectors with a fixed number of dimensions, often clustering related data points using unsupervised learning methods. This enables low latency queries, even for massive datasets, which greatly increases efficiency. Vector embeddings also store each vector’s metadata, further enhancing search possibilities.\\n\\nLangChain provides integrations for over 25 different embedding methods, as well as for over 50 different vector stores (both cloud-hosted and local).\\n\\nText splitters To increase speed and reduce computational demands, it’s often wise to split large text documents into smaller pieces. LangChain’s TextSplitters split text up into small, semantically meaningful chunks that can then be combined using methods and parameters of your choosing.\\n\\nRetrieval Once external sources of knowledge have been connected, the model must be able to quickly retrieve and integrate relevant information as needed. Like watsonx, LangChain offers retrieval augmented generation (RAG): its retriever modules accept a string query as an input and return a list of Document’s as output.\\n\\nLLMs, by default, do not have any long-term memory of prior conversations (unless that chat history is used as input for a query). LangChain solves this problem with simple utilities for adding memory to a system, with options ranging from retaining the entirety of all conversations to retaining a summarization of the conversation thus far to retaining the n most recent exchanges.\\n\\nLangChain agents can use a given language model as a “reasoning engine” to determine which actions to take. When building a chain for an agent, inputs include:\\n\\na list of available tools to be leveraged.\\n\\nuser input (like prompts and queries).\\n\\nany relevant previously executed steps.\\n\\nLearn more about agents in LangChain\\n\\nDespite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\n\\nLangChain tools (link resides outside ibm.com) are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Examples of prominent LangChain tools include:\\n\\nWolfram Alpha: provides access to powerful computational and data visualization functions, enabling sophisticated mathematical capabilities.\\n\\nGoogle Search: provides access to Google Search, equipping applications and agents with real-time information.\\n\\nOpenWeatherMap: fetches weather information.\\n\\nWikipedia: provides efficient access to information from Wikipedia articles.\\n\\nReleased in the fall of 2023, LangSmith aims to bridge the gap between the accessible prototyping capabilities that brought LangChain to prominence and building production-quality LLM applications.\\n\\nLangSmith provides tools to monitor, evaluate and debug applications, including the ability to automatically trace all model calls to spot errors and test performance under different model configurations. This visibility aims to empower more robust, cost-efficient applications.\\n\\nGetting started with LangChain\\n\\nLangChain is open source and free to use: source code is available for download on Github (link resides outside ibm.com).\\n\\nLangChain can also be installed on Python with a simple pip command: pip install langchain. To install all LangChain dependencies (rather than only those you find necessary), you can run the command pip install langchain[all].\\n\\nMany step-by-step tutorials are available from both the greater LangChain community ecosystem and the official documentation at docs.langchain.com (link resides outside ibm.com).\\n\\nApplications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a “reasoning engine.”\\n\\nChatbots: Chatbots are among the most intuitive uses of LLMs. LangChain can be used to provide proper context for the specific use of a chatbot, and to integrate chatbots into existing communication channels and workflows with their own APIs.\\n\\nSummarization: Language models can be tasked with summarizing many types of text, from breaking down complex academic articles and transcripts to providing a digest of incoming emails.\\n\\nQuestion answering: Using specific documents or specialized knowledge bases (like Wolfram, arXiv or PubMed), LLMs can retrieve relevant information from storage and articulate helpful answers). If fine-tuned or properly prompted, some LLMs can answer many questions even without external information.\\n\\nData augmentation: LLMs can be used to generate synthetic data for use in machine learning. For example, an LLM can be trained to generate additional data samples that closely resemble the data points in a training dataset.\\n\\nVirtual agents: Integrated with the right workflows, LangChain’s Agent modules can use an LLM to autonomously determine next steps and take action using robotic process automation (RPA).\\n\\nTrain, validate, tune and deploy generative AI, foundation models and machine learning capabilities with ease and build AI applications in a fraction of the time with a fraction of the data.\\n\\nAI consulting services\\n\\nReimagine how you work with AI: our diverse, global team of more than 20,000 AI experts can help you quickly and confidently design and scale AI and automation across your business, working across our own IBM watsonx technology and an open ecosystem of partners to deliver any AI model, on any cloud, guided by ethics and trust.\\n\\nExplore IBM AI consulting services\\n\\nScale analytics and AI workloads for all your data, anywhere with watsonx.data, the industry’s only data store that is open, hybrid and governed.\\n\\nExplore watsonx.data\\n\\nTools, tips and sample code to begin building applications with LangChain and watsonx.\\n\\nTips for writing foundation model prompts\\n\\nPart art, part science, prompt engineering is the process of crafting prompt text to best effect for a given model and parameters. These tips will help you successfully prompt most text-generating foundation models.\\n\\nUse watsonx and LangChain to call a language model\\n\\nThis notebook contains the steps and code to demonstrate Simple Sequential Chain using langchain integration with watsonx models. Some familiarity with Python is helpful.\\n\\nA beginner's guide to Python\\n\\nWe'll introduce the basic concepts you need to know to get started with this straightforward programming language, from running algebraic calculations to generating graphical output from your data.\\n\\nTrain, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data.\\n\\n1 The fastest-growing open-source startups in Q2 2023 (link resides outside ibm.com), Runa Capital, 2023\", 'timestamp': '2024-03-16T06:58:09', 'title': 'What Is LangChain? | IBM', 'url': 'https://www.ibm.com/topics/langchain'}, {'id': 'web-search_1', 'snippet': \"🦜️🔗 LangChain\\n\\n⚡ Build context-aware reasoning applications ⚡\\n\\nLooking for the JS/TS library? Check out LangChain.js.\\n\\nTo help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building, testing, and monitoring LLM applications. Fill out this form to speak with our sales team.\\n\\npip install langchain\\n\\nconda install langchain -c conda-forge\\n\\n🤔 What is LangChain?\\n\\nLangChain is a framework for developing applications powered by language models. It enables applications that:\\n\\nAre context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\\n\\nReason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)\\n\\nThis framework consists of several parts.\\n\\nLangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents.\\n\\nLangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks.\\n\\nLangServe: A library for deploying LangChain chains as a REST API.\\n\\nLangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.\\n\\nLangGraph: LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain. It extends the LangChain Expression Language with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner.\\n\\nThe LangChain libraries themselves are made up of several different packages.\\n\\nlangchain-core: Base abstractions and LangChain Expression Language.\\n\\nlangchain-community: Third party integrations.\\n\\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\n\\n🧱 What can you build with LangChain?\\n\\n❓ Retrieval augmented generation\\n\\nEnd-to-end Example: Chat LangChain and repo\\n\\n💬 Analyzing structured data\\n\\nEnd-to-end Example: SQL Llama2 Template\\n\\nEnd-to-end Example: Web LangChain (web researcher chatbot) and repo\\n\\nAnd much more! Head to the Use cases section of the docs for more.\\n\\n🚀 How does LangChain help?\\n\\nThe main value props of the LangChain libraries are:\\n\\nComponents: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\\n\\nOff-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks\\n\\nOff-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.\\n\\nComponents fall into the following modules:\\n\\nThis includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\\n\\nData Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\\n\\nPlease see here for full documentation, which includes:\\n\\nGetting started: installation, setting up the environment, simple examples\\n\\nOverview of the interfaces, modules, and integrations\\n\\nUse case walkthroughs and best practice guides\\n\\nLangSmith, LangServe, and LangChain Template overviews\\n\\nReference: full API docs\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see here.\", 'timestamp': '2024-03-21T15:00:04', 'title': 'GitHub - langchain-ai/langchain: 🦜🔗 Build context-aware reasoning applications', 'url': 'https://github.com/langchain-ai/langchain'}, {'id': 'web-search_9', 'snippet': \"Skip to main content\\n\\nLangChain is a framework for developing applications powered by language models. It enables applications that:\\n\\nAre context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\\n\\nReason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)\\n\\nThis framework consists of several parts.\\n\\nLangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents.\\n\\nLangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks.\\n\\nLangServe: A library for deploying LangChain chains as a REST API.\\n\\nLangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.\\n\\nTogether, these products simplify the entire application lifecycle:\\n\\nDevelop: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.\\n\\nProductionize: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.\\n\\nDeploy: Turn any chain into an API with LangServe.\\n\\nLangChain Libraries\\u200b\\n\\nThe main value props of the LangChain packages are:\\n\\nComponents: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\\n\\nOff-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks\\n\\nOff-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.\\n\\nThe LangChain libraries themselves are made up of several different packages.\\n\\nlangchain-core: Base abstractions and LangChain Expression Language.\\n\\nlangchain-community: Third party integrations.\\n\\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\n\\nHere’s how to install LangChain, set up your environment, and start building.\\n\\nWe recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.\\n\\nRead up on our Security best practices to make sure you're developing safely with LangChain.\\n\\nThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\n\\nLangChain Expression Language (LCEL)\\u200b\\n\\nLCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains.\\n\\nOverview: LCEL and its benefits\\n\\nInterface: The standard interface for LCEL objects\\n\\nHow-to: Key features of LCEL\\n\\nCookbook: Example code for accomplishing common tasks\\n\\nLangChain provides standard, extendable interfaces and integrations for the following modules:\\n\\nInterface with language models\\n\\nInterface with application-specific data\\n\\nLet models choose which tools to use given high-level directives\\n\\nExamples, ecosystem, and resources\\u200b\\n\\nWalkthroughs and techniques for common end-to-end use cases, like:\\n\\nDocument question answering\\n\\nAnalyzing structured data\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.\\n\\nBest practices for developing with LangChain.\\n\\nHead to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.\\n\\nDeveloper's guide\\u200b\\n\\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\\n\\nHelp us out by providing feedback on this documentation page:\\n\\nLangChain Expression Language (LCEL)\\n\\nExamples, ecosystem, and resources\", 'timestamp': '2024-03-20T21:18:24', 'title': 'Introduction | 🦜️🔗 Langchain', 'url': 'https://python.langchain.com/docs/get_started/introduction'}, {'id': 'web-search_4', 'snippet': \"Skip to main content Switch to mobile version\\n\\npip install langchain Copy PIP instructions\\n\\nReleased: Mar 13, 2024\\n\\nBuilding applications with LLMs through composability\\n\\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\\n\\nLicense: MIT License (MIT)\\n\\nRequires: Python >=3.8.1, <4.0\\n\\nMaintainers hwchase17\\n\\nOSI Approved :: MIT License\\n\\nProgramming Language\\n\\n🦜️🔗 LangChain\\n\\n⚡ Building applications with LLMs through composability ⚡\\n\\nLooking for the JS/TS version? Check out LangChain.js.\\n\\nTo help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building, testing, and monitoring LLM applications. Fill out this form to speak with our sales team.\\n\\npip install langchain or pip install langsmith && conda install langchain -c conda-forge\\n\\nLarge language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. However, using these LLMs in isolation is often insufficient for creating a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.\\n\\nThis library aims to assist in the development of those types of applications. Common examples of these applications include:\\n\\n❓ Question Answering over specific documents\\n\\nEnd-to-end Example: Question Answering over Notion Database\\n\\nEnd-to-end Example: Chat-LangChain\\n\\nEnd-to-end Example: GPT+WolframAlpha\\n\\nPlease see here for full documentation on:\\n\\nGetting started (installation, setting up the environment, simple examples)\\n\\nHow-To examples (demos, integrations, helper functions)\\n\\nReference (full API docs)\\n\\nResources (high-level explanation of core concepts)\\n\\n🚀 What can this help with?\\n\\nThere are six main areas that LangChain is designed to help with. These are, in increasing order of complexity:\\n\\n📃 LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\\n\\nChains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\n\\n📚 Data Augmented Generation:\\n\\nData Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\\n\\nMemory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\n\\n[BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\n\\nFor more information on these concepts, please see our full documentation.\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the Contributing Guide.\\n\\nView statistics for this project via Libraries.io, or by using our public dataset on Google BigQuery\\n\\nLicense: MIT License (MIT)\\n\\nRequires: Python >=3.8.1, <4.0\\n\\nMaintainers hwchase17\\n\\nOSI Approved :: MIT License\\n\\nProgramming Language\\n\\nRelease history Release notifications | RSS feed\\n\\n0.0.349rc2 pre-release\\n\\n0.0.349rc1 pre-release\\n\\n0.0.339rc3 pre-release\\n\\n0.0.339rc2 pre-release\\n\\n0.0.339rc1 pre-release\\n\\n0.0.339rc0 pre-release\\n\\n0.0.331rc3 pre-release\\n\\n0.0.331rc2 pre-release\\n\\n0.0.331rc1 pre-release\\n\\n0.0.331rc0 pre-release\\n\\n0.0.240rc4 pre-release\\n\\n0.0.240rc1 pre-release\\n\\n0.0.240rc0 pre-release\\n\\n0.0.102rc0 pre-release\\n\\n0.0.101rc0 pre-release\\n\\n0.0.99rc0 pre-release\\n\\nDownload the file for your platform. If you're not sure which to choose, learn more about installing packages.\\n\\nlangchain-0.1.12.tar.gz (412.0 kB view hashes)\\n\\nUploaded Mar 13, 2024 source\\n\\nlangchain-0.1.12-py3-none-any.whl (809.1 kB view hashes)\\n\\nUploaded Mar 13, 2024 py3\\n\\nHashes for langchain-0.1.12.tar.gz\\n\\nHashes for langchain-0.1.12.tar.gz\\n\\n5f612761ba548b81748ed8dc70535e8de0531445415028a82de3fd8255bfa8a3\\n\\n05a305f1700d3db21d1cf78554cdb13c\\n\\nab3e2f051b14f17916d261012297a15008ad0b4f12a5183d740fd2b835fc4952\\n\\nHashes for langchain-0.1.12-py3-none-any.whl\\n\\nHashes for langchain-0.1.12-py3-none-any.whl\\n\\nb4dd1760e2d035daefad08af60a209b96b729ee45492d34e3e127e553a471034\\n\\n3b694e00db59fe137caf545ebd6641e8\\n\\nb0589a8777dff52bf5485c391cf3951e3d3b787afdc5967b29e25694ea014377\\n\\nAWS Cloud computing and Security Sponsor Datadog Monitoring Fastly CDN Google Download Analytics Microsoft PSF Sponsor Pingdom Monitoring Sentry Error logging StatusPage Status page\", 'timestamp': '2024-03-19T16:15:30', 'title': 'langchain · PyPI', 'url': 'https://pypi.org/project/langchain/'}, {'id': 'web-search_12', 'snippet': '🦜️🏓 LangServe\\n\\n🚩 We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist.\\n\\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\\n\\nThis library is integrated with FastAPI and uses pydantic for data validation.\\n\\nIn addition, it provides a client that can be used to call into runnables deployed on a server. A JavaScript client is available in LangChain.js.\\n\\nInput and Output schemas automatically inferred from your LangChain object, and enforced on every API call, with rich error messages\\n\\nAPI docs page with JSONSchema and Swagger (insert example link)\\n\\nEfficient /invoke/, /batch/ and /stream/ endpoints with support for many concurrent requests on a single server\\n\\n/stream_log/ endpoint for streaming all (or some) intermediate steps from your chain/agent\\n\\nnew as of 0.0.40, supports astream_events to make it easier to stream without needing to parse the output of stream_log.\\n\\nPlayground page at /playground/ with streaming output and intermediate steps\\n\\nBuilt-in (optional) tracing to LangSmith, just add your API key (see Instructions)\\n\\nAll built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop and asyncio.\\n\\nUse the client SDK to call a LangServe server as if it was a Runnable running locally (or call the HTTP API directly)\\n\\nClient callbacks are not yet supported for events that originate on the server\\n\\nOpenAPI docs will not be generated when using Pydantic V2. Fast API does not support mixing pydantic v1 and v2 namespaces. See section below for more details.\\n\\nWe will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist.\\n\\nVulnerability in Versions 0.0.13 - 0.0.15 -- playground endpoint allows accessing arbitrary files on server. Resolved in 0.0.16.\\n\\nFor both client and server:\\n\\npip install \"langserve[all]\"\\n\\nor pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.\\n\\nLangChain CLI 🛠️\\n\\nUse the LangChain CLI to bootstrap a LangServe project quickly.\\n\\nTo use the langchain CLI make sure that you have a recent version of langchain-cli installed. You can install it with pip install -U langchain-cli.\\n\\nlangchain app new ../path/to/directory\\n\\nGet your LangServe instance started quickly with LangChain Templates.\\n\\nFor more examples, see the templates index or the examples directory.\\n\\nLLMs Minimal example that reserves OpenAI and Anthropic chat models. Uses async, supports batching and streaming.\\n\\nRetriever Simple server that exposes a retriever as a runnable.\\n\\nConversational Retriever A Conversational Retriever exposed via LangServe\\n\\nAgent without conversation history based on OpenAI tools\\n\\nAgent with conversation history based on OpenAI tools\\n\\nRunnableWithMessageHistory to implement chat persisted on backend, keyed off a session_id supplied by client.\\n\\nRunnableWithMessageHistory to implement chat persisted on backend, keyed off a conversation_id supplied by client, and user_id (see Auth for implementing user_id properly).\\n\\nConfigurable Runnable to create a retriever that supports run time configuration of the index name.\\n\\nConfigurable Runnable that shows configurable fields and configurable alternatives.\\n\\nAPIHandler Shows how to use APIHandler instead of add_routes. This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort.\\n\\nLCEL Example Example that uses LCEL to manipulate a dictionary input.\\n\\nAuth with add_routes: Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.)\\n\\nAuth with add_routes: Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.)\\n\\nAuth with add_routes: Implement per user logic and auth for endpoints that use per request config modifier. (Note: At the moment, does not integrate with OpenAPI docs.)\\n\\nAuth with APIHandler: Implement per user logic and auth that shows how to search only within user owned documents.\\n\\nWidgets Different widgets that can be used with playground (file upload and chat)\\n\\nWidgets File upload widget used for LangServe playground.\\n\\nHere\\'s a server that deploys an OpenAI chat model, an Anthropic chat model, and a chain that uses the Anthropic model to tell a joke about a topic.\\n\\n#!/usr/bin/env python from fastapi import FastAPI from langchain.prompts import ChatPromptTemplate from langchain.chat_models import ChatAnthropic, ChatOpenAI from langserve import add_routes app = FastAPI( title=\"LangChain Server\", version=\"1.0\", description=\"A simple api server using Langchain\\'s Runnable interfaces\", ) add_routes( app, ChatOpenAI(), path=\"/openai\", ) add_routes( app, ChatAnthropic(), path=\"/anthropic\", ) model = ChatAnthropic() prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") add_routes( app, prompt | model, path=\"/joke\", ) if __name__ == \"__main__\": import uvicorn uvicorn.run(app, host=\"localhost\", port=8000)\\n\\nIf you intend to call your endpoint from the browser, you will also need to set CORS headers. You can use FastAPI\\'s built-in middleware for that:\\n\\nfrom fastapi.middleware.cors import CORSMiddleware # Set all CORS enabled origins app.add_middleware( CORSMiddleware, allow_origins=[\"*\"], allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"], expose_headers=[\"*\"], )\\n\\nIf you\\'ve deployed the server above, you can view the generated OpenAPI docs using:\\n\\n⚠️ If using pydantic v2, docs will not be generated for invoke, batch, stream, stream_log. See Pydantic section below for more details.\\n\\ncurl localhost:8000/docs\\n\\nmake sure to add the /docs suffix.\\n\\n⚠️ Index page / is not defined by design, so curl localhost:8000 or visiting the URL will return a 404. If you want content at / define an endpoint @app.get(\"/\").\\n\\nfrom langchain.schema import SystemMessage, HumanMessage from langchain.prompts import ChatPromptTemplate from langchain.schema.runnable import RunnableMap from langserve import RemoteRunnable openai = RemoteRunnable(\"http://localhost:8000/openai/\") anthropic = RemoteRunnable(\"http://localhost:8000/anthropic/\") joke_chain = RemoteRunnable(\"http://localhost:8000/joke/\") joke_chain.invoke({\"topic\": \"parrots\"}) # or async await joke_chain.ainvoke({\"topic\": \"parrots\"}) prompt = [ SystemMessage(content=\\'Act like either a cat or a parrot.\\'), HumanMessage(content=\\'Hello!\\') ] # Supports astream async for msg in anthropic.astream(prompt): print(msg, end=\"\", flush=True) prompt = ChatPromptTemplate.from_messages( [(\"system\", \"Tell me a long story about {topic}\")] ) # Can define custom chains chain = prompt | RunnableMap({ \"openai\": openai, \"anthropic\": anthropic, }) chain.batch([{\"topic\": \"parrots\"}, {\"topic\": \"cats\"}])\\n\\nIn TypeScript (requires LangChain.js version 0.0.166 or later):\\n\\nimport { RemoteRunnable } from \"@langchain/core/runnables/remote\"; const chain = new RemoteRunnable({ url: `http://localhost:8000/joke/`, }); const result = await chain.invoke({ topic: \"cats\", });\\n\\nPython using requests:\\n\\nimport requests response = requests.post( \"http://localhost:8000/joke/invoke\", json={\\'input\\': {\\'topic\\': \\'cats\\'}} ) response.json()\\n\\nYou can also use curl:\\n\\ncurl --location --request POST \\'http://localhost:8000/joke/invoke\\' \\\\ --header \\'Content-Type: application/json\\' \\\\ --data-raw \\'{ \"input\": { \"topic\": \"cats\" } }\\'\\n\\n... add_routes( app, runnable, path=\"/my_runnable\", )\\n\\nadds of these endpoints to the server:\\n\\nPOST /my_runnable/invoke - invoke the runnable on a single input\\n\\nPOST /my_runnable/batch - invoke the runnable on a batch of inputs\\n\\nPOST /my_runnable/stream - invoke on a single input and stream the output\\n\\nPOST /my_runnable/stream_log - invoke on a single input and stream the output, including output of intermediate steps as it\\'s generated\\n\\nPOST /my_runnable/astream_events - invoke on a single input and stream events as they are generated, including from intermediate steps.\\n\\nGET /my_runnable/input_schema - json schema for input to the runnable\\n\\nGET /my_runnable/output_schema - json schema for output of the runnable\\n\\nGET /my_runnable/config_schema - json schema for config of the runnable\\n\\nThese endpoints match the LangChain Expression Language interface -- please reference this documentation for more details.\\n\\nYou can find a playground page for your runnable at /my_runnable/playground/. This exposes a simple UI to configure and invoke your runnable with streaming output and intermediate steps.\\n\\nThe playground supports widgets and can be used to test your runnable with different inputs. See the widgets section below for more details.\\n\\nIn addition, for configurable runnables, the playground will allow you to configure the runnable and share a link with the configuration:\\n\\nLangServe also makes a chat-focused playground available at /my_runnable/chat_playground/. Unlike the general playground, only certain types of runnables are supported - the runnable\\'s input schema must be a dict with a single key, and that key\\'s value must be a list of chat messages. The runnable can return either an AIMessage or a string.\\n\\nHere\\'s an example route:\\n\\n# Declare a chain prompt = ChatPromptTemplate.from_messages( [ (\"system\", \"You are a helpful, professional assistant named Cob.\"), MessagesPlaceholder(variable_name=\"messages\"), ] ) chain = prompt | ChatAnthropic(model=\"claude-2\") class InputChat(BaseModel): \"\"\"Input for the chat endpoint.\"\"\" messages: List[Union[HumanMessage, AIMessage, SystemMessage]] = Field( ..., description=\"The chat messages representing the current conversation.\", ) add_routes( app, chain.with_types(input_type=InputChat), enable_feedback_endpoint=True, enable_public_trace_link_endpoint=True, )\\n\\nIf you are using LangSmith, you can also set enable_feedback_endpoint=True on your route to enable thumbs-up/thumbs-down buttons after each message, and enable_public_trace_link_endpoint=True to add a button that creates a public traces for runs. Note that you will also need to set the following environment variables:\\n\\nexport LANGCHAIN_TRACING_V2=\"true\" export LANGCHAIN_PROJECT=\"YOUR_PROJECT_NAME\" export LANGCHAIN_API_KEY=\"YOUR_API_KEY\"\\n\\nHere\\'s an example with the above two options turned on:\\n\\nNote: If you enable public trace links, the internals of your chain will be exposed. We recommend only using this setting for demos or testing.\\n\\nLangServe works with both Runnables (constructed via LangChain Expression Language) and legacy chains (inheriting from Chain). However, some of the input schemas for legacy chains may be incomplete/incorrect, leading to errors. This can be fixed by updating the input_schema property of those chains in LangChain. If you encounter any errors, please open an issue on THIS repo, and we will work to address it.\\n\\nYou can deploy to AWS using the AWS Copilot CLI\\n\\ncopilot init --app [application-name] --name [service-name] --type \\'Load Balanced Web Service\\' --dockerfile \\'./Dockerfile\\' --deploy\\n\\nClick here to learn more.\\n\\nYou can deploy to Azure using Azure Container Apps (Serverless):\\n\\naz containerapp up --name [container-app-name] --source . --resource-group [resource-group-name] --environment [environment-name] --ingress external --target-port 8001 --env-vars=OPENAI_API_KEY=your_key\\n\\nYou can find more info here\\n\\nYou can deploy to GCP Cloud Run using the following command:\\n\\ngcloud run deploy [your-service-name] --source . --port 8001 --allow-unauthenticated --region us-central1 --set-env-vars=OPENAI_API_KEY=your_key\\n\\nDeploy using Infrastructure as Code\\n\\nYou can deploy your LangServe server with Pulumi using your preferred general purpose language. Below are some quickstart examples for deploying LangServe to different cloud providers.\\n\\nThese examples are a good starting point for your own infrastructure as code (IaC) projects. You can easily modify them to suit your needs.\\n\\nhttps://github.com/pulumi/examples/aws-cs-langserve\\n\\nhttps://github.com/pulumi/examples/aws-go-langserve\\n\\nhttps://github.com/pulumi/examples/aws-py-langserve\\n\\nhttps://github.com/pulumi/examples/aws-ts-langserve\\n\\nhttps://github.com/pulumi/examples/aws-js-langserve\\n\\nCommunity Contributed\\n\\nExample Railway Repo\\n\\nLangServe provides support for Pydantic 2 with some limitations.\\n\\nOpenAPI docs will not be generated for invoke/batch/stream/stream_log when using Pydantic V2. Fast API does not support [mixing pydantic v1 and v2 namespaces].\\n\\nLangChain uses the v1 namespace in Pydantic v2. Please read the following guidelines to ensure compatibility with LangChain\\n\\nExcept for these limitations, we expect the API endpoints, the playground and any other features to work as expected.\\n\\nHandling Authentication\\n\\nIf you need to add authentication to your server, please read Fast API\\'s documentation about dependencies and security.\\n\\nThe below examples show how to wire up authentication logic LangServe endpoints using FastAPI primitives.\\n\\nYou are responsible for providing the actual authentication logic, the users table etc.\\n\\nIf you\\'re not sure what you\\'re doing, you could try using an existing solution Auth0.\\n\\nIf you\\'re using add_routes, see examples here.\\n\\nAuth with add_routes: Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.)\\n\\nAuth with add_routes: Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.)\\n\\nAuth with add_routes: Implement per user logic and auth for endpoints that use per request config modifier. (Note: At the moment, does not integrate with OpenAPI docs.)\\n\\nAlternatively, you can use FastAPI\\'s middleware.\\n\\nUsing global dependencies and path dependencies has the advantage that auth will be properly supported in the OpenAPI docs page, but these are not sufficient for implement per user logic (e.g., making an application that can search only within user owned documents).\\n\\nIf you need to implement per user logic, you can use the per_req_config_modifier or APIHandler (below) to implement this logic.\\n\\nIf you need authorization or logic that is user dependent, specify per_req_config_modifier when using add_routes. Use a callable receives the raw Request object and can extract relevant information from it for authentication and authorization purposes.\\n\\nIf you feel comfortable with FastAPI and python, you can use LangServe\\'s APIHandler.\\n\\nAuth with APIHandler: Implement per user logic and auth that shows how to search only within user owned documents.\\n\\nAPIHandler Shows how to use APIHandler instead of add_routes. This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort.\\n\\nIt\\'s a bit more work, but gives you complete control over the endpoint definitions, so you can do whatever custom logic you need for auth.\\n\\nLLM applications often deal with files. There are different architectures that can be made to implement file processing; at a high level:\\n\\nThe file may be uploaded to the server via a dedicated endpoint and processed using a separate endpoint\\n\\nThe file may be uploaded by either value (bytes of file) or reference (e.g., s3 url to file content)\\n\\nThe processing endpoint may be blocking or non-blocking\\n\\nIf significant processing is required, the processing may be offloaded to a dedicated process pool\\n\\nYou should determine what is the appropriate architecture for your application.\\n\\nCurrently, to upload files by value to a runnable, use base64 encoding for the file (multipart/form-data is not supported yet).\\n\\nHere\\'s an example that shows how to use base64 encoding to send a file to a remote runnable.\\n\\nRemember, you can always upload files by reference (e.g., s3 url) or upload them as multipart/form-data to a dedicated endpoint.\\n\\nCustom Input and Output Types\\n\\nInput and Output types are defined on all runnables.\\n\\nYou can access them via the input_schema and output_schema properties.\\n\\nLangServe uses these types for validation and documentation.\\n\\nIf you want to override the default inferred types, you can use the with_types method.\\n\\nHere\\'s a toy example to illustrate the idea:\\n\\nfrom typing import Any from fastapi import FastAPI from langchain.schema.runnable import RunnableLambda app = FastAPI() def func(x: Any) -> int: \"\"\"Mistyped function that should accept an int but accepts anything.\"\"\" return x + 1 runnable = RunnableLambda(func).with_types( input_type=int, ) add_routes(app, runnable)\\n\\nInherit from CustomUserType if you want the data to de-serialize into a pydantic model rather than the equivalent dict representation.\\n\\nAt the moment, this type only works server side and is used to specify desired decoding behavior. If inheriting from this type the server will keep the decoded type as a pydantic model instead of converting it into a dict.\\n\\nfrom fastapi import FastAPI from langchain.schema.runnable import RunnableLambda from langserve import add_routes from langserve.schema import CustomUserType app = FastAPI() class Foo(CustomUserType): bar: int def func(foo: Foo) -> int: \"\"\"Sample function that expects a Foo type which is a pydantic model\"\"\" assert isinstance(foo, Foo) return foo.bar # Note that the input and output type are automatically inferred! # You do not need to specify them. # runnable = RunnableLambda(func).with_types( # <-- Not needed in this case # input_type=Foo, # output_type=int, # add_routes(app, RunnableLambda(func), path=\"/foo\")\\n\\nThe playground allows you to define custom widgets for your runnable from the backend.\\n\\nHere are a few examples:\\n\\nWidgets Different widgets that can be used with playground (file upload and chat)\\n\\nWidgets File upload widget used for LangServe playground.\\n\\nA widget is specified at the field level and shipped as part of the JSON schema of the input type\\n\\nA widget must contain a key called type with the value being one of a well known list of widgets\\n\\nOther widget keys will be associated with values that describe paths in a JSON object\\n\\ntype JsonPath = number | string | (number | string)[]; type NameSpacedPath = { title: string; path: JsonPath }; // Using title to mimick json schema, but can use namespace type OneOfPath = { oneOf: JsonPath[] }; type Widget = { type: string // Some well known type (e.g., base64file, chat etc.) [key: string]: JsonPath | NameSpacedPath | OneOfPath; };\\n\\nThere are only two widgets that the user can specify manually right now:\\n\\nSee below more information about these widgets.\\n\\nAll other widgets on the playground UI are created and managed automatically by the UI based on the config schema of the Runnable. When you create Configurable Runnables, the playground should create appropriate widgets for you to control the behavior.\\n\\nAllows creation of a file upload input in the UI playground for files that are uploaded as base64 encoded strings. Here\\'s the full example.\\n\\ntry: from pydantic.v1 import Field except ImportError: from pydantic import Field from langserve import CustomUserType # ATTENTION: Inherit from CustomUserType instead of BaseModel otherwise # the server will decode it into a dict instead of a pydantic model. class FileProcessingRequest(CustomUserType): \"\"\"Request including a base64 encoded file.\"\"\" # The extra field is used to specify a widget for the playground UI. file: str = Field(..., extra={\"widget\": {\"type\": \"base64file\"}}) num_chars: int = 100\\n\\nLook at the widget example.\\n\\nTo define a chat widget, make sure that you pass \"type\": \"chat\".\\n\\n\"input\" is JSONPath to the field in the Request that has the new input message.\\n\\n\"output\" is JSONPath to the field in the Response that has new output message(s).\\n\\nDon\\'t specify these fields if the entire input or output should be used as they are ( e.g., if the output is a list of chat messages.)\\n\\nclass ChatHistory(CustomUserType): chat_history: List[Tuple[str, str]] = Field( ..., examples=[[(\"human input\", \"ai response\")]], extra={\"widget\": {\"type\": \"chat\", \"input\": \"question\", \"output\": \"answer\"}}, ) question: str def _format_to_messages(input: ChatHistory) -> List[BaseMessage]: \"\"\"Format the input to a list of messages.\"\"\" history = input.chat_history user_input = input.question messages = [] for human, ai in history: messages.append(HumanMessage(content=human)) messages.append(AIMessage(content=ai)) messages.append(HumanMessage(content=user_input)) return messages model = ChatOpenAI() chat_model = RunnableParallel({\"answer\": (RunnableLambda(_format_to_messages) | model)}) add_routes( app, chat_model.with_types(input_type=ChatHistory), config_keys=[\"configurable\"], path=\"/chat\", )\\n\\nYou can also specify a list of messages as your a parameter directly, as shown in this snippet:\\n\\nprompt = ChatPromptTemplate.from_messages( [ (\"system\", \"You are a helpful assisstant named Cob.\"), MessagesPlaceholder(variable_name=\"messages\"), ] ) chain = prompt | ChatAnthropic(model=\"claude-2\") class MessageListInput(BaseModel): \"\"\"Input for the chat endpoint.\"\"\" messages: List[Union[HumanMessage, AIMessage]] = Field( ..., description=\"The chat messages representing the current conversation.\", extra={\"widget\": {\"type\": \"chat\", \"input\": \"messages\"}}, ) add_routes( app, chain.with_types(input_type=MessageListInput), path=\"/chat\", )\\n\\nSee this sample file for an example.\\n\\nEnabling / Disabling Endpoints (LangServe >=0.0.33)\\n\\nYou can enable / disable which endpoints are exposed when adding routes for a given chain.\\n\\nUse enabled_endpoints if you want to make sure to never get a new endpoint when upgrading langserve to a newer verison.\\n\\nEnable: The code below will only enable invoke, batch and the corresponding config_hash endpoint variants.\\n\\nadd_routes(app, chain, enabled_endpoints=[\"invoke\", \"batch\", \"config_hashes\"], path=\"/mychain\")\\n\\nDisable: The code below will disable the playground for the chain\\n\\nadd_routes(app, chain, disabled_endpoints=[\"playground\"], path=\"/mychain\")', 'timestamp': '2024-03-09T04:02:49', 'title': 'GitHub - langchain-ai/langserve: LangServe 🦜️🏓', 'url': 'https://github.com/langchain-ai/langserve'}, {'id': 'web-search_14', 'snippet': 'Your first A.I. API endpoint with 🦜LangServe\\n\\nDeploying your first A.I. Rest API Endpoint with LangServe is EASY! We’ll walk through everything to get your first LangServe project online.\\n\\nLangChain has been at the forefront of providing developers with tools that enhance productivity and open up new avenues for AI integration.\\n\\nLangServe API Endpoint Deployment\\n\\nThe latest addition to their ecosystem, LangServe, is no exception. Designed to simplify the deployment of LangChain runnables and chains as REST APIs, LangServe is designed for developers leveraging AI in their applications. This guide will walk you through setting up your first LangServe project, making the process as simple as possible.\\n\\nIntroducing LangServe\\n\\nUnderstanding LangServe\\n\\nBefore we dive into the setup process, let’s take a moment to understand what LangServe brings to the table:\\n\\nIntegration with FastAPI: At its core, LangServe leverages FastAPI to offer a robust and speedy API development experience. FastAPI’s intuitive design pairs perfectly with LangServe’s goals.\\n\\nData Validation with Pydantic: Data integrity is paramount, and LangServe uses Pydantic for data validation, ensuring that your inputs and outputs are exactly as expected.\\n\\nLangChainJS: For those who prefer working with JavaScript, LangServe hasn’t left you out. The LangChainJS client allows seamless interaction with LangServe endpoints.\\n\\nAutomatic schema inference, robust API documentation, and efficient handling of concurrent requests are just the start.\\n\\nThe /stream_log/ endpoint and astream_events feature cater to those who need real-time data streaming and logging.\\n\\nIntegration with LangSmith for tracing, alongside the support of powerful Python libraries, makes LangServe not just powerful but also incredibly versatile.\\n\\nPreparing for Your First LangServe Project\\n\\nGetting started with LangServe is straightforward. Here’s what you need:\\n\\nInstallation Requirements: Ensure you have Python (I used 3.11) and pip installed. For Mac users, brew will be needed to install gh.\\n\\nSetting Up the Environment\\n\\nInstall the langchain-cli tool with pip:\\n\\npip install -U langchain-cli\\n\\n2. Install gh using brew for easy GitHub integration:\\n\\n3. Create a new LangServe project using the example pirate-speak template:\\n\\nlangchain app new langserve-demo --package pirate-speak\\n\\nConfiguring Your LangServe Project\\n\\nOnce your project is created, you’ll need to make some tweaks to get it running:\\n\\nEdit Your Project: Open app/server.py in VSCode. Replace the placeholder add_routes(app, NotImplemented) with the output code provided during the template installation:\\n\\nfrom pirate_speak.chain import chain as pirate_speak_chain add_routes(app, pirate_speak_chain, path=\"/pirate-speak\")\\n\\n2. Test Locally: Run langchain serve from the app root directory and visit http://127.0.0.1:8000 to see your project live.\\n\\nPublishing Your Project to Hosted LangServe\\n\\nReady to share your project with the world? Here’s how:\\n\\nInitial Setup on GitHub: Initialize your git repository, commit your changes:\\n\\ngit init git add . git commit -m \"Initial Commit\"\\n\\n2. Push the changes to GitHub using gh.\\n\\ngh repo create #- push an existing local repository to github #- path to local repo (.) #- Name the repo: langserve-demo #- set repo owner #- Description #- Pub/Private #- Add a remote (Y) #- What should the new remote be called? \\'origin\\' #- Would you like to push the commits from the current branch to origin? (y)\\n\\n3. View Your Repository: Confirm everything’s in place by viewing your repository online with gh.\\n\\nFinal Step: Deploying your LangServe Project\\n\\nLangServe Access: Head over to your LangSmith Beta Account and find the LangServe Alpha section for “Deployments” in the left-hand navigation pane. If you do not have access to LangSmith or LangServe and want to get started now, DM me.\\n\\nLangSmith is a platform for building production-grade LLM applications. It lets you debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework and seamlessly integrates with LangChain, the go-to open source framework for building with LLMs.www.langsmith.com\\n\\nCreate new LangServe deployment interface\\n\\n2. Deploy Your Project: Connect your GitHub account\\n\\nGet started by clicking the “New Deployment” button in the upper right corner of your browser.\\n\\nFirst, you’ll need to link your GitHub account. Follow the onscreen instructions to link your existing GitHub account with LangSmith to allow access to all repositories for easier deployment.\\n\\nOnce GitHub is linked, search for the repo name we created earlier, “langserve-demo” and select the repo.\\n\\nNext, name your LangServe deployment. Use lowercase letters and (-) only for naming conventions.\\n\\nIn the next field, you’ll select your subdirectory. For this example deployment, leave this as the default (.).\\n\\nFor the next field, “Git reference”, leave this as the default which is populated by your repo.\\n\\nLastly you’ll need to create an environment variable that contains your OpenAI API Key.\\n\\n3. In the “name” field, enter exactly: OPENAI_API_KEY\\n\\n4. In the “value” field, enter your OpenAI API Key from your OpenAI Developer account control panel.\\n\\nThe final step is to click “Submit” in the upper right-hand corner to deploy your new LangServe FastAPI API Endpoint.\\n\\nDeployment can take a little bit of time. Expect 4–7 minutes for deployment to complete at the time of this writing.\\n\\nExploring the LangServe API\\n\\nOnce your LangServe project is up and running, two powerful tools at your disposal will significantly enhance your development experience: the LangServe Playground and FastAPI Documentation. These tools are not just about testing; they’re about exploring the potential of your AI endpoint and understanding its capabilities in depth.\\n\\nLangServe Playground\\n\\nThe LangServe Playground is a feature designed to let developers experiment with their deployed AI endpoints. It provides a user-friendly interface for sending prompts to your API and viewing the responses in real-time. This immediate feedback loop is invaluable for prompt engineering, allowing for rapid iteration and refinement of your AI models.\\n\\nLangServe Playground\\n\\nAccessing the Playground: To dive into the Playground, navigate to the LangSmith Deployments Projects listing. Find your deployed project, and append your LangServe project name followed by /playground to the link provided. For instance:\\n\\nhttps://langserve-demo-9722-ffoprvkqsa-uc.a.run.app/pirate-speak/playground/\\n\\nThis URL will take you directly to the Playground of your pirate-speak endpoint, where you can start testing your prompts and seeing how the AI responds. The Playground is an excellent tool for both beginners and experienced developers to experiment with different inputs and fine-tune their AI\\'s behavior.\\n\\nFastAPI Documentation\\n\\nLangServe leverages FastAPI to offer comprehensive and interactive API documentation automatically. This documentation is easily accessible and provides a clear overview of your API’s capabilities, request formats, and response structures. It’s a valuable resource for developers looking to integrate their LangServe API endpoints into applications or for those who wish to understand the technical details of their deployed services.\\n\\nLangServe FastAPI Docs\\n\\nAccessing the API Docs: Simply visit the default URL of your LangServe API. The FastAPI-generated documentation allows you to explore available endpoints, try out requests, and view the expected responses. This interactive documentation is an excellent way for developers to familiarize themselves with the API’s functionality and ensure their applications are correctly integrated.\\n\\nLeveraging These Tools\\n\\nThe LangServe Playground and FastAPI documentation provides a robust set of tools for exploring, testing, and refining your AI API endpoints. They offer a hands-on approach to understanding how your AI models perform under different scenarios and how they can be integrated into broader applications. Whether you’re fine-tuning prompts in the Playground or diving into the technical details with the FastAPI docs, these resources are designed to enhance your development workflow and ship faster 🚀.\\n\\nNext Steps and Resources\\n\\nWith your LangServe AI endpoint now deployed, the possibilities are endless. Explore other LangServe templates, join the LangChain community for support, and don’t hesitate to experiment with your newly acquired skills.\\n\\nBe Happy and Learn - follow these X Accounts @GitMaxd @LangChainAI @hwchase17 @Hacubu\\n\\nIf you’ve found this article useful, give me a like and a follow @ https://x.com/gitmaxd — I follow back!\\n\\nWritten by Rick Garcia\\n\\nLinux Dev | AI Revolution Early Adopter | Love ReactJS/Next.js | Founder 3x startups (+$15mm) | Currently building amazing things at http://Slicie.com', 'timestamp': '2024-03-22T07:03:55', 'title': 'Your first A.I. API endpoint with 🦜LangServe | by Rick Garcia | Feb, 2024 | Medium', 'url': 'https://medium.com/@gitmaxd/your-first-a-i-api-endpoint-with-langserve-deeb65e750b1'}, {'id': 'web-search_0', 'snippet': \"Applications that can reason. Powered by LangChain.\\n\\nLangChain’s suite of products supports developers along each step of the LLM application lifecycle.\\n\\nRequest a demoGo to Docs\\n\\nFrom startups to global enterprises, ambitious builders choose LangChain.\\n\\nLangChain gives developers a framework to construct LLM‑powered apps easily.\\n\\nLangSmith gives visibility into what’s happening with your LLM-powered app, whether it's built with LangChain or not, so you know how to take action and improve quality.\\n\\nLangServe makes serving an API for your LangChain application turnkey.\\n\\nBuild your app with LangChain\\n\\nBuild context-aware, reasoning applications with LangChain’s flexible framework that leverages your company’s data and APIs. Future-proof your application by making vendor optionality part of your LLM infrastructure design.\\n\\nLearn more about LangChain\\n\\nObserve performance with LangSmith\\n\\nShip faster with LangSmith’s debug, test, deploy, and monitoring workflows. Don’t rely on “vibes” – add engineering rigor to your LLM-development workflow, whether you’re building with LangChain or not.\\n\\nLearn more about LangSmith\\n\\nInstantly deploy with LangServe\\n\\nEasily deploy your LangChain application with LangServe, with built in parallelization, fallbacks, batch, streaming, and async support for your API endpoints.\\n\\nLearn more about LangServe\\n\\nHear from our happy customers\\n\\nLangSmith helps teams of all sizes, across all industries, from ambitious startups to established enterprises.\\n\\n“LangSmith helped us improve the accuracy and performance of Retool’s fine-tuned models. Not only did we deliver a better product by iterating with LangSmith, but we’re shipping new AI features to our users in a fraction of the time it would have taken without it.”\\n\\nHead of Self-Serve and New Products\\n\\n“By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches of using LLMs in an enterprise-setting faster.”\\n\\nGeneral Manager of AI\\n\\n“Working with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the development and shipping experience. We couldn’t have achieved the product experience delivered to our customers without LangChain, and we couldn’t have done it at the same pace without LangSmith.”\\n\\nDirector of Security Products\\n\\n“As soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.”\\n\\n“LangSmith helped us improve the accuracy and performance of Retool’s fine-tuned models. Not only did we deliver a better product by iterating with LangSmith, but we’re shipping new AI features to our users in a fraction of the time it would have taken without it.”\\n\\nHead of Self-Serve and New Products\\n\\n“By combining the benefits of LangSmith and standing on the shoulders of a gigantic open-source community, we’re able to identify the right approaches of using LLMs in an enterprise-setting faster.”\\n\\nGeneral Manager of AI\\n\\n“Working with LangChain and LangSmith on the Elastic AI Assistant had a significant positive impact on the overall pace and quality of the development and shipping experience. We couldn’t have achieved the product experience delivered to our customers without LangChain, and we couldn’t have done it at the same pace without LangSmith.”\\n\\nDirector of Security Products\\n\\n“As soon as we heard about LangSmith, we moved our entire development stack onto it. We could have built evaluation, testing and monitoring tools in house, but with LangSmith it took us 10x less time to get a 1000x better tool.”\\n\\nThe reference architecture enterprises adopt for success.\\n\\nLangChain's suite of products easily stack together for ease of set up and multiplicative impact.\\n\\nThe biggest developer community in GenAI\\n\\nLearn alongside the 100K+ practitioners who are pushing the industry forward.\\n\\nGet started with the LangSmith platform today\\n\\nCreate an accountSchedule a demo\\n\\nTeams building with LangChain are driving operational efficiency, increasing discovery & personalization, and delivering premium products that generate revenue.\\n\\nGet inspired by companies who have done it.\\n\\nLangSmith is the enterprise DevOps platform built for LLMs.\\n\\nGain visibility to make trade offs between cost, latency, and quality.\\n\\nIncrease developer productivity.\\n\\nEliminate manual, error-prone testing.\\n\\nReduce hallucinations and improve reliability.\\n\\nEnterprise deployment options to keep data secure.\\n\\nReady to start shipping reliable GenAI apps faster?\\n\\nLangChain and LangSmith are critical parts of the reference architecture to get you from prototype to production.\\n\\nRequest a demoSign Up\", 'timestamp': '2024-03-18T16:57:57', 'title': 'LangChain', 'url': 'https://www.langchain.com/'}, {'id': 'web-search_13', 'snippet': 'Introducing LangServe, the best way to deploy your LangChains\\n\\nBy LangChain 5 min read Oct 12, 2023\\n\\nWe think the LangChain Expression Language (LCEL) is the quickest way to prototype the brains of your LLM application. The next exciting step is to ship it to your users and get some feedback! Today we\\'re making that a lot easier, launching LangServe. LangServe is the easiest and best way to deploy any any LangChain chain/agent/runnable.\\n\\nLangServe Github Repo\\n\\nExample repo (deploy on GCP)\\n\\nWhether you’re building a customer-facing chatbot, or an internal tool powered by LLMs, you’ll probably start by building a prototype (maybe in a Jupyter notebook), and iterate on it until it’s good enough to get people using it.\\n\\nAnd that’s where LangServe comes in, we’ve taken our experience scaling applications in production, and made it available as a python package you can use for your own LLM apps.\\n\\nYour Idea + LCEL = Prototype + LangServe = Production-ready API + [Hosting Provider] = Live deployment + LangSmith Tracing = Monitor your production deployment\\n\\nWe\\'ve been hesitant to release anything around deployment because, to be honest, we weren\\'t entirely sure what value we could add. With some key improvements and additions over the past few months, we now believe that LangServe can provide real value. Most of the features we highlight in the release today make it significantly easier to go from prototype to production and get a production ready API. Over next few weeks, we\\'ll be adding more features that add in new functionality (as opposed to just making it easier to create an API).\\n\\nWe\\'ve used LangServe to deploy ChatLangChain and WebLangChain over the past week, and this has help battle test it.\\n\\nThe road to LangServe\\n\\nThe journey to build LangServe really started a few months ago when we launched LCEL and the Runnable protocol. This was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully running in production LCEL chains with 100s of steps). To highlight a few:\\n\\nfirst-class support for streaming: when you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens. We’re constantly improving streaming support, recently we added a streaming JSON parser, and more is in the works.\\n\\nfirst-class async support: any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.\\n\\noptimized parallel execution: whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\\n\\nsupport for retries and fallbacks: more recently we’ve added support for configuring retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\\n\\naccessing intermediate results: for more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used let end-users know something is happening, or even just to debug your chain. We’ve added support for streaming intermediate results, and it’s available on every LangServe server.\\n\\ninput and output schemas: this week we launched input and output schemas for LCEL, giving every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\\n\\nTo this ever-growing list of the benefits of building with LCEL today we’re adding LangServe, which takes advantage of all these to give you the fastest path from LLM idea to scalable LLM app in production.\\n\\nFirst we create our chain, here using a conversational retrieval chain, but any other chain would work. This is the my_package/chain.py file.\\n\\n\"\"\"A conversational retrieval chain.\"\"\" from langchain.chains import ConversationalRetrievalChain from langchain.chat_models import ChatOpenAI from langchain.embeddings import OpenAIEmbeddings from langchain.vectorstores import FAISS vectorstore = FAISS.from_texts( [\"cats like fish\", \"dogs like sticks\"], embedding=OpenAIEmbeddings() ) retriever = vectorstore.as_retriever() model = ChatOpenAI() chain = ConversationalRetrievalChain.from_llm(model, retriever)\\n\\nThen, we pass that chain to add_routes. This is the my_package/server.py file.\\n\\n#!/usr/bin/env python \"\"\"A server for the chain above.\"\"\" from fastapi import FastAPI from langserve import add_routes from my_package.chain import chain app = FastAPI(title=\"Retrieval App\") add_routes(app, chain) if __name__ == \"__main__\": import uvicorn uvicorn.run(app, host=\"localhost\", port=8000)\\n\\nThat\\'s it! This gets you a scalable python web server with\\n\\nInput and Output schemas automatically inferred from your chain, and enforced on every API call, with rich error messages\\n\\n/docs endpoint serves API docs with JSONSchema and Swagger (insert example link)\\n\\n/invoke endpoint that accepts JSON input and returns JSON output from your chain, with support for many concurrent requests in the same server\\n\\n/batch endpoint that produces output for several inputs in parallel, batching calls to LLMs where possible\\n\\n/stream endpoint that sends output chunks as they become available, using SSE (same as OpenAI Streaming API)\\n\\n/stream_log endpoint for streaming all (or some) intermediate steps from your chain/agent\\n\\nBuilt-in (optional) tracing to LangSmith, just add your API key as an environment variable\\n\\nSupport for hosting multiple chains in the same server under separate paths\\n\\nAll built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop and asyncio.\\n\\nAfter an API is created, the next step is to deploy it on a hosting platform. We\\'re launching with two examples:\\n\\nGCP: Deploy to GCP Cloud Run with one command.\\n\\nReplit: Deploy (and also build) on Replit. This can be done by cloning an existing Github repo.\\n\\nWe will add instructions and templates for other platforms. If you have suggestions for what you want to see, please let us know.\\n\\nLet\\'s see it in action. We have deployed this repo as an example.\\n\\nThe API docs are available here.\\n\\nWe can stream a response:\\n\\ncurl https://langserve-launch-example-vz4y4ooboq-uc.a.run.app/stream -X POST -H \"Content-Type: application/json\" --data \\'{\"input\": {\"topic\": \"bears\"}}\\'\\n\\nWe\\'re always improving LangChain/LCEL, just recently we\\'ve added support for input and output schemas, streaming intermediate results, and a streaming JSON parser.\\n\\nWe\\'ll also be working to add features over the next few weeks: the next two we are adding are (1) a playground to experiment with different prompts/retrievers for deployed chains, (2) a way to save multiple configurations for the same chain.\\n\\nLet us know what you\\'d like to see next!\\n\\nUpdates from the LangChain team and community', 'timestamp': '2024-03-08T15:26:12', 'title': 'Introducing LangServe, the best way to deploy your LangChains', 'url': 'https://blog.langchain.dev/introducing-langserve/'}, {'id': 'web-search_15', 'snippet': 'Want to Become a Sponsor? Contact Us Now!🎉\\n\\nHow to Use Langchain with Chroma, the Open Source Vector Database\\n\\nLangChain Embeddings - Tutorial & Examples for LLMs\\n\\nHow to Load Json Files in Langchain - A Step-by-Step Guide\\n\\nHow to Give LLM Conversational Memory with LangChain - Getting Started with LangChain Memory\\n\\nLangServe: Tutorial for Easy LangChain Deployment\\n\\nLangSmith: Best Way to Test LLMs and AI Application\\n\\n[LangChain Tutorial] How to Add Memory to load_qa_chain and Answer Questions\\n\\nHow to Use Vector Store in LangChain to Chat with Documents (with Steps)\\n\\nLangServe: Tutorial for Easy LangChain Deployment\\n\\nLangServe: Tutorial for Easy LangChain Deployment\\n\\nPublished on 12/17/2023\\n\\nLangServe is not just another tool in the LangChain ecosystem; it\\'s a game-changer. If you\\'ve been grappling with the complexities of deploying LangChain runnables and chains, LangServe is the magic wand you\\'ve been waiting for. This article aims to be your ultimate guide, focusing on how to use LangServe for seamless LangChain deployment.\\n\\nThe importance of LangServe cannot be overstated. It\\'s the bridge that takes your LangChain projects from the development stage to real-world applications. Whether you\\'re a seasoned developer or a newbie in the LangChain universe, mastering LangServe is crucial. So, let\\'s dive in.\\n\\nWhat is LangServe and Why You Should Care\\n\\nLangServe is a Python package designed to make LangChain deployment as smooth as butter. It allows you to deploy any LangChain runnable or chain as a REST API, effectively turning your LangChain projects into production-ready applications.\\n\\nAutomatic Schema Inference: No more manual labor for defining input and output schemas. LangServe does it for you.\\n\\nAPI Endpoints: It comes with built-in API endpoints like /invoke, /batch, and /stream that can handle multiple requests concurrently.\\n\\nMonitoring: With built-in LangSmith tracing, you can keep an eye on your deployments in real-time.\\n\\nWhy Should You Care?\\n\\nIf you\\'re in the LangChain ecosystem, LangServe is indispensable for several reasons:\\n\\nSimplifies Deployment: LangServe eliminates the need for complex configurations, making it easier for you to focus on your core logic.\\n\\nScalability: It\\'s designed to scale, meaning as your LangChain project grows, LangServe grows with it.\\n\\nTime-Saving: With features like automatic schema inference and efficient API endpoints, LangServe saves you a ton of time, speeding up your project\\'s time-to-market.\\n\\nIn essence, LangServe is not just a tool; it\\'s your deployment partner. It takes the guesswork out of LangChain deployments, letting you focus on what you do best: creating amazing LangChain projects.\\n\\nKey Features of LangServe in LangChain Deployment\\n\\nLangServe comes packed with features that make it the go-to solution for LangChain deployment. Here\\'s a rundown of its key features:\\n\\nAutomatic Input and Output Schema Inference: LangServe automatically infers the input and output schemas from your LangChain object. This eliminates the need for manual schema definitions, making your life a whole lot easier.\\n\\nEfficient API Endpoints: LangServe provides you with efficient API endpoints like /invoke, /batch, and /stream. These endpoints are designed to handle multiple concurrent requests, ensuring that your LangChain application can serve multiple users simultaneously without breaking a sweat.\\n\\nBuilt-in Monitoring with LangSmith: One of the standout features of LangServe is its built-in tracing to LangSmith. This allows you to monitor your LangChain deployments in real-time, providing valuable insights into the performance and health of your application.\\n\\nEach of these features is designed to simplify and streamline the process of deploying your LangChain projects. Whether you\\'re deploying a simple chatbot or a complex data analysis tool, LangServe has got you covered.\\n\\nSetting Up LangServe for LangChain Deployment: A Step-by-Step Guide\\n\\nPre-requisites for LangServe Setup\\n\\nBefore diving into the LangServe setup, it\\'s essential to ensure you have the right environment. Here\\'s what you\\'ll need:\\n\\nPython 3.8 or higher: LangServe is a Python package, so you\\'ll need Python installed on your system.\\n\\nLangChain CLI: This is the command-line interface for LangChain, which you\\'ll use to install LangServe.\\n\\nGit: You\\'ll need Git for cloning example repositories.\\n\\nOnce you have these in place, you\\'re ready to install LangServe and start deploying your LangChain projects.\\n\\nInstalling LangServe\\n\\nInstalling LangServe is a breeze, thanks to the LangChain CLI. Open your terminal and run the following command:\\n\\nlangchain-cli install langserve\\n\\nThis command fetches the latest version of LangServe and installs it on your system. Once the installation is complete, you can verify it by running:\\n\\nIf you see the version number, congratulations! You\\'ve successfully installed LangServe.\\n\\nCreating Your First LangChain Runnable\\n\\nNow that LangServe is installed, let\\'s create our first LangChain runnable. A runnable is essentially a piece of code that performs a specific task in your LangChain project. Here\\'s a sample code snippet to set up a basic LangChain runnable:\\n\\nfrom langchain import Runnable class MyRunnable(Runnable): def run(self, input_data): return {\"output\": input_data[\"input\"] * 2}\\n\\nSave this code in a file named my_runnable.py.\\n\\nDeploying Your Runnable with LangServe\\n\\nWith your runnable in place, it\\'s time to deploy it using LangServe. Create a new Python file named deploy.py and add the following code:\\n\\nfrom fastapi import FastAPI from langserve import add_routes from my_runnable import MyRunnable app = FastAPI() runnable = MyRunnable() add_routes(app, runnable)\\n\\nThis code sets up a FastAPI application and adds routes for your runnable using LangServe\\'s add_routes function.\\n\\nTo run your FastAPI application, execute the following command:\\n\\nuvicorn deploy:app --reload\\n\\nYour LangChain runnable is now deployed as a REST API, accessible at http://localhost:8000.\\n\\nTesting Your Deployment\\n\\nAfter deploying your runnable, it\\'s crucial to test it to ensure everything is working as expected. Use curl or Postman to send a POST request to http://localhost:8000/invoke with the following JSON payload:\\n\\nIf everything is set up correctly, you should receive a JSON response with the output value of 10.\\n\\nDeployment Options for LangServe\\n\\nDeploying LangServe on GCP Cloud Run\\n\\nGoogle Cloud Platform (GCP) is one of the most popular cloud services, and LangServe makes it incredibly easy to deploy your LangChain projects on GCP Cloud Run. Here\\'s how:\\n\\nBuild a Docker Image: Create a Dockerfile in your project directory with the following content:\\n\\nFROM python:3.8 COPY . /app WORKDIR /app RUN pip install -r requirements.txt CMD [\"uvicorn\", \"deploy:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\\n\\ndocker build -t my-langserve-app .\\n\\nDeploy to Cloud Run:\\n\\ngcloud run deploy --image gcr.io/your-project-id/my-langserve-app\\n\\nAnd that\\'s it! Your LangChain project is now deployed on GCP Cloud Run.\\n\\nDeploying LangServe on Replit\\n\\nReplit is an excellent platform for quick prototyping, and you can also use it to deploy LangServe. Simply clone your LangServe project repository into Replit and hit the \"Run\" button. Replit automatically detects the FastAPI application and deploys it.\\n\\nFuture Developments in LangServe\\n\\nLangServe is not a static tool; it\\'s continuously evolving to meet the growing demands of the LangChain ecosystem. While it already offers a robust set of features for LangChain deployment, the development team has plans to take it even further. Here\\'s a sneak peek into what\\'s coming:\\n\\nSupport for More Cloud Platforms: While LangServe currently supports deployment on GCP Cloud Run and Replit, future updates aim to include compatibility with other cloud platforms like AWS and Azure.\\n\\nEnhanced Monitoring Capabilities: LangServe\\'s built-in tracing to LangSmith is just the tip of the iceberg. Upcoming releases plan to offer more in-depth analytics and monitoring features to help you keep a closer eye on your deployments.\\n\\nAdvanced API Features: The development team is working on adding more advanced API features, including real-time data streaming and batch processing capabilities, to make LangServe even more powerful.\\n\\nThese future developments are designed to make LangServe an even more indispensable tool for LangChain deployment. Whether you\\'re a solo developer or part of a large team, these upcoming features promise to make your life easier and your deployments more robust.\\n\\nAdditional Resources for LangServe and LangChain Deployment\\n\\nWhile this guide aims to be comprehensive, LangServe and LangChain have a lot more to offer. Here are some additional resources that can help you deepen your understanding and skills:\\n\\nLangChain Deployment GitHub Repositories: There are several example repositories available on GitHub that demonstrate different types of LangChain deployments. These are excellent resources for learning and can serve as templates for your projects.\\n\\nLangChain Server Documentation: For those who want to delve deeper into the technical aspects, the LangChain server documentation is a treasure trove of information. It covers everything from basic setup to advanced features.\\n\\nLangChain Discord Community: If you have questions or run into issues, the LangChain Discord community is a great place to seek help. It\\'s also a fantastic platform for networking with other LangChain developers and staying updated on the latest news and updates.\\n\\nLangServe Github (opens in a new tab)\\n\\nLangServe API (opens in a new tab)\\n\\nLangServe is a groundbreaking tool that simplifies the complex task of LangChain deployment. From its key features to a detailed step-by-step setup guide, this article has armed you with the knowledge you need to start deploying your LangChain projects like a pro. With LangServe, the power to scale and deploy is right at your fingertips.\\n\\nAs LangServe continues to evolve, so do the opportunities for creating more robust and scalable LangChain deployments. So, whether you\\'re just starting out or looking to take your existing projects to the next level, LangServe is the tool you\\'ve been waiting for.\\n\\nHow to Give LLM Conversational Memory with LangChain - Getting Started with LangChain MemoryLangSmith: Best Way to Test LLMs and AI Application', 'timestamp': '2024-02-27T06:27:29', 'title': 'LangServe: Tutorial for Easy LangChain Deployment – AI StartUps Product Information, Reviews, Latest Updates', 'url': 'https://cheatsheet.md/langchain-tutorials/langserve-for-langchain-deployment'}, {'id': 'web-search_10', 'snippet': 'Skip to main content\\n\\nLangSmith helps you trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\nCheck out the interactive walkthrough to get started.\\n\\nFor more information, please refer to the LangSmith documentation.\\n\\nFor tutorials and other end-to-end examples demonstrating ways to integrate LangSmith in your workflow, check out the LangSmith Cookbook. Some of the guides therein include:\\n\\nLeveraging user feedback in your JS application (link).\\n\\nBuilding an automated feedback pipeline (link).\\n\\nHow to evaluate and audit your RAG workflows (link).\\n\\nHow to fine-tune an LLM on real usage data (link).\\n\\nHow to use the LangChain Hub to version your prompts (link)\\n\\nHelp us out by providing feedback on this documentation page:', 'timestamp': '2024-03-08T16:37:16', 'title': 'LangSmith | 🦜️🔗 Langchain', 'url': 'https://python.langchain.com/docs/langsmith/'}, {'id': 'web-search_11', 'snippet': 'Skip to main content\\n\\nLangSmith Walkthrough\\n\\nLangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will have to iterate on your prompts, chains, and other components to build a high-quality product.\\n\\nLangSmith makes it easy to debug, test, and continuously improve your LLM applications.\\n\\nWhen might this come in handy? You may find it useful when you want to:\\n\\nQuickly debug a new chain, agent, or set of tools\\n\\nCreate and manage datasets for fine-tuning, few-shot prompting, and evaluation\\n\\nRun regression tests on your application to confidently develop\\n\\nCapture production analytics for product insights and continuous improvements\\n\\nCreate a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docs\\n\\nNote LangSmith is in closed beta; we’re in the process of rolling it out to more users. However, you can fill out the form on the website for expedited access.\\n\\nNow, let’s get started!\\n\\nLog runs to LangSmith\\u200b\\n\\nFirst, configure your environment variables to tell LangChain to log traces. This is done by setting the LANGCHAIN_TRACING_V2 environment variable to true. You can tell LangChain which project to log to by setting the LANGCHAIN_PROJECT environment variable (if this isn’t set, runs will be logged to the default project). This will automatically create the project for you if it doesn’t exist. You must also set the LANGCHAIN_ENDPOINT and LANGCHAIN_API_KEY environment variables.\\n\\nFor more information on other ways to set up tracing, please reference the LangSmith documentation.\\n\\nNOTE: You can also use a context manager in python to log traces using\\n\\nfrom langchain_core.tracers.context import tracing_v2_enabled with tracing_v2_enabled(project_name=\"My Project\"): agent.run(\"How many people live in canada as of 2023?\")\\n\\nHowever, in this example, we will use environment variables.\\n\\n%pip install --upgrade --quiet langchain langsmith langchainhub --quiet %pip install --upgrade --quiet langchain-openai tiktoken pandas duckduckgo-search --quiet\\n\\nimport os from uuid import uuid4 unique_id = uuid4().hex[0:8] os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\" os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" os.environ[\"LANGCHAIN_API_KEY\"] = \"<YOUR-API-KEY>\" # Update to your API key # Used by the agent in this tutorial os.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"\\n\\nCreate the langsmith client to interact with the API\\n\\nfrom langsmith import Client client = Client()\\n\\nCreate a LangChain component and log runs to the platform. In this example, we will create a ReAct-style agent with access to a general search tool (DuckDuckGo). The agent’s prompt can be viewed in the Hub here.\\n\\nfrom langchain import hub from langchain.agents import AgentExecutor from langchain.agents.format_scratchpad.openai_tools import ( format_to_openai_tool_messages, ) from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser from langchain_community.tools import DuckDuckGoSearchResults from langchain_openai import ChatOpenAI # Fetches the latest version of this prompt prompt = hub.pull(\"wfh/langsmith-agent-prompt:5d466cbc\") llm = ChatOpenAI( model=\"gpt-3.5-turbo-16k\", temperature=0, ) tools = [ DuckDuckGoSearchResults( name=\"duck_duck_go\" ), # General internet search using DuckDuckGo ] llm_with_tools = llm.bind_tools(tools) runnable_agent = ( { \"input\": lambda x: x[\"input\"], \"agent_scratchpad\": lambda x: format_to_openai_tool_messages( x[\"intermediate_steps\"] ), } | prompt | llm_with_tools | OpenAIToolsAgentOutputParser() ) agent_executor = AgentExecutor( agent=runnable_agent, tools=tools, handle_parsing_errors=True )\\n\\nWe are running the agent concurrently on multiple inputs to reduce latency. Runs get logged to LangSmith in the background so execution latency is unaffected.\\n\\ninputs = [ \"What is LangChain?\", \"What\\'s LangSmith?\", \"When was Llama-v2 released?\", \"What is the langsmith cookbook?\", \"When did langchain first announce the hub?\", ] results = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)\\n\\n[{\\'input\\': \\'What is LangChain?\\', \\'output\\': \\'I\\\\\\'m sorry, but I couldn\\\\\\'t find any information about \"LangChain\". Could you please provide more context or clarify your question?\\'}, {\\'input\\': \"What\\'s LangSmith?\", \\'output\\': \\'I\\\\\\'m sorry, but I couldn\\\\\\'t find any information about \"LangSmith\". It could be a company, a product, or a person. Can you provide more context or details about what you are referring to?\\'}]\\n\\nAssuming you’ve successfully set up your environment, your agent traces should show up in the Projects section in the app. Congrats!\\n\\nIt looks like the agent isn’t effectively using the tools though. Let’s evaluate this so we have a baseline.\\n\\nIn addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications.\\n\\nIn this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps:\\n\\nInitialize a new agent to benchmark\\n\\nConfigure evaluators to grade an agent’s output\\n\\nRun the agent over the dataset and evaluate the results\\n\\n1. Create a LangSmith dataset\\u200b\\n\\nBelow, we use the LangSmith client to create a dataset from the input questions from above and a list labels. You will use these later to measure performance for a new agent. A dataset is a collection of examples, which are nothing more than input-output pairs you can use as test cases to your application.\\n\\nFor more information on datasets, including how to create them from CSVs or other files or how to create them in the platform, please refer to the LangSmith documentation.\\n\\noutputs = [ \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\", \"LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain\", \"July 18, 2023\", \"The langsmith cookbook is a github repository containing detailed examples of how to use LangSmith to debug, evaluate, and monitor large language model-powered applications.\", \"September 5, 2023\", ]\\n\\ndataset_name = f\"agent-qa-{unique_id}\" dataset = client.create_dataset( dataset_name, description=\"An example dataset of questions over the LangSmith documentation.\", ) client.create_examples( inputs=[{\"input\": query} for query in inputs], outputs=[{\"output\": answer} for answer in outputs], dataset_id=dataset.id, )\\n\\n2. Initialize a new agent to benchmark\\u200b\\n\\nLangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn’t shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.\\n\\nIn this case, we will test an agent that uses OpenAI’s function calling endpoints.\\n\\nfrom langchain import hub from langchain.agents import AgentExecutor, AgentType, initialize_agent, load_tools from langchain_openai import ChatOpenAI # Since chains can be stateful (e.g. they can have memory), we provide # a way to initialize a new chain for each row in the dataset. This is done # by passing in a factory function that returns a new chain for each row. def create_agent(prompt, llm_with_tools): runnable_agent = ( { \"input\": lambda x: x[\"input\"], \"agent_scratchpad\": lambda x: format_to_openai_tool_messages( x[\"intermediate_steps\"] ), } | prompt | llm_with_tools | OpenAIToolsAgentOutputParser() ) return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)\\n\\n3. Configure evaluation\\u200b\\n\\nManually comparing the results of chains in the UI is effective, but it can be time consuming. It can be helpful to use automated metrics and AI-assisted feedback to evaluate your component’s performance.\\n\\nBelow, we will create a custom run evaluator that logs a heuristic evaluation.\\n\\nHeuristic evaluators\\n\\nfrom langsmith.evaluation import EvaluationResult from langsmith.schemas import Example, Run def check_not_idk(run: Run, example: Example): \"\"\"Illustration of a custom evaluator.\"\"\" agent_response = run.outputs[\"output\"] if \"don\\'t know\" in agent_response or \"not sure\" in agent_response: score = 0 else: score = 1 # You can access the dataset labels in example.outputs[key] # You can also access the model inputs in run.inputs[key] return EvaluationResult( key=\"not_uncertain\", score=score, )\\n\\nSome metrics are aggregated over a full “test” without being assigned to an individual runs/examples. These could be as simple as common classification metrics like Precision, Recall, or AUC, or it could be another custom aggregate metric.\\n\\nYou can define any batch metric on a full test level by defining a function (or any callable) that accepts a list of Runs (system traces) and list of Examples (dataset records).\\n\\nfrom typing import List def max_pred_length(runs: List[Run], examples: List[Example]): predictions = [len(run.outputs[\"output\"]) for run in runs] return EvaluationResult(key=\"max_pred_length\", score=max(predictions))\\n\\nBelow, we will configure the evaluation with the custom evaluator from above, as well as some pre-implemented run evaluators that do the following: - Compare results against ground truth labels. - Measure semantic (dis)similarity using embedding distance - Evaluate ‘aspects’ of the agent’s response in a reference-free manner using custom criteria\\n\\nFor a longer discussion of how to select an appropriate evaluator for your use case and how to create your own custom evaluators, please refer to the LangSmith documentation.\\n\\nfrom langchain.evaluation import EvaluatorType from langchain.smith import RunEvalConfig evaluation_config = RunEvalConfig( # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator evaluators=[ check_not_idk, # Measures whether a QA response is \"Correct\", based on a reference answer # You can also select via the raw string \"qa\" EvaluatorType.QA, # Measure the embedding distance between the output and the reference answer # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings()) EvaluatorType.EMBEDDING_DISTANCE, # Grade whether the output satisfies the stated criteria. # You can select a default one such as \"helpfulness\" or provide your own. RunEvalConfig.LabeledCriteria(\"helpfulness\"), # The LabeledScoreString evaluator outputs a score on a scale from 1-10. # You can use default criteria or write our own rubric RunEvalConfig.LabeledScoreString( { \"accuracy\": \"\"\" Score 1: The answer is completely unrelated to the reference. Score 3: The answer has minor relevance but does not align with the reference. Score 5: The answer has moderate relevance but contains inaccuracies. Score 7: The answer aligns with the reference but has minor errors or omissions. Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\" }, normalize_by=10, ), ], batch_evaluators=[max_pred_length], )\\n\\n4. Run the agent and evaluators\\u200b\\n\\nUse the run_on_dataset (or asynchronous arun_on_dataset) function to evaluate your model. This will: 1. Fetch example rows from the specified dataset. 2. Run your agent (or any custom function) on each example. 3. Apply evaluators to the resulting run traces and corresponding reference examples to generate automated feedback.\\n\\nThe results will be visible in the LangSmith app.\\n\\nfrom langchain import hub # We will test this version of the prompt prompt = hub.pull(\"wfh/langsmith-agent-prompt:798e7324\")\\n\\nimport functools from langchain.smith import arun_on_dataset, run_on_dataset chain_results = run_on_dataset( dataset_name=dataset_name, llm_or_chain_factory=functools.partial( create_agent, prompt=prompt, llm_with_tools=llm_with_tools ), evaluation=evaluation_config, verbose=True, client=client, project_name=f\"tools-agent-test-5d466cbc-{unique_id}\", # Project metadata communicates the experiment parameters, # Useful for reviewing the test results project_metadata={ \"env\": \"testing-notebook\", \"model\": \"gpt-3.5-turbo\", \"prompt\": \"5d466cbc\", }, ) # Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc. # These are logged as warnings here and captured as errors in the tracing UI.\\n\\nReview the test results\\u200b\\n\\nYou can review the test results tracing UI below by clicking the URL in the output above or navigating to the “Testing & Datasets” page in LangSmith “agent-qa-{unique_id}” dataset.\\n\\nThis will show the new runs and the feedback logged from the selected evaluators. You can also explore a summary of the results in tabular format below.\\n\\nchain_results.to_dataframe()\\n\\n(Optional) Compare to another prompt\\u200b\\n\\nNow that we have our test run results, we can make changes to our agent and benchmark them. Let’s try this again with a different prompt and see the results.\\n\\ncandidate_prompt = hub.pull(\"wfh/langsmith-agent-prompt:39f3bbd0\") chain_results = run_on_dataset( dataset_name=dataset_name, llm_or_chain_factory=functools.partial( create_agent, prompt=candidate_prompt, llm_with_tools=llm_with_tools ), evaluation=evaluation_config, verbose=True, client=client, project_name=f\"tools-agent-test-39f3bbd0-{unique_id}\", project_metadata={ \"env\": \"testing-notebook\", \"model\": \"gpt-3.5-turbo\", \"prompt\": \"39f3bbd0\", }, )\\n\\nExporting datasets and runs\\u200b\\n\\nLangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client to fetch runs for further analysis, to store in your own database, or to share with others. Let’s fetch the run traces from the evaluation run.\\n\\nNote: It may be a few moments before all the runs are accessible.\\n\\nruns = client.list_runs(project_name=chain_results[\"project_name\"], execution_order=1)\\n\\n# The resulting tests are stored in a project. You can programmatically # access important metadata from the test, such as the dataset version it was run on # or your application\\'s revision ID. client.read_project(project_name=chain_results[\"project_name\"]).metadata\\n\\n# After some time, the test metrics will be populated as well. client.read_project(project_name=chain_results[\"project_name\"]).feedback_stats\\n\\nCongratulations! You have successfully traced and evaluated an agent using LangSmith!\\n\\nThis was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results.\\n\\nFor more information on how you can get the most out of LangSmith, check out LangSmith documentation, and please reach out with questions, feature requests, or feedback at support@langchain.dev.\\n\\nHelp us out by providing feedback on this documentation page:\\n\\nLog runs to LangSmith\\n\\n1. Create a LangSmith dataset\\n\\n2. Initialize a new agent to benchmark\\n\\n3. Configure evaluation\\n\\n4. Run the agent and evaluators\\n\\nReview the test results\\n\\n(Optional) Compare to another prompt\\n\\nExporting datasets and runs', 'timestamp': '2024-03-21T10:39:52', 'title': 'LangSmith Walkthrough | 🦜️🔗 Langchain', 'url': 'https://python.langchain.com/docs/langsmith/walkthrough'}, {'id': 'web-search_18', 'snippet': \"Want to Become a Sponsor? Contact Us Now!🎉\\n\\nHow to Use Langchain with Chroma, the Open Source Vector Database\\n\\nLangChain Embeddings - Tutorial & Examples for LLMs\\n\\nHow to Load Json Files in Langchain - A Step-by-Step Guide\\n\\nHow to Give LLM Conversational Memory with LangChain - Getting Started with LangChain Memory\\n\\nLangServe: Tutorial for Easy LangChain Deployment\\n\\nLangSmith: Best Way to Test LLMs and AI Application\\n\\n[LangChain Tutorial] How to Add Memory to load_qa_chain and Answer Questions\\n\\nHow to Use Vector Store in LangChain to Chat with Documents (with Steps)\\n\\nLangSmith: Best Way to Test LLMs and AI Application\\n\\nLangSmith: Best Way to Test LLMs and AI Application\\n\\nPublished on 12/17/2023\\n\\nIf you're in the world of Language Learning Models (LLMs), you've probably heard of LangSmith. But do you know how it can transform your LLM applications from good to great? This article is your one-stop guide to understanding LangSmith, a platform that offers a plethora of features for debugging, testing, evaluating, and monitoring LLM applications.\\n\\nWhether you're a seasoned developer or a beginner in the field of LLMs, LangSmith has something for everyone. From its seamless integration with LangChain to its robust Cookbook filled with real-world examples, LangSmith is a game-changer. Let's dive in!\\n\\nLangSmith is a cutting-edge platform designed to elevate your LLM applications to production-grade quality. But what does that mean? In simple terms, LangSmith is your toolkit for building, testing, and deploying intelligent agents and chains based on any LLM framework. It's developed by LangChain, the same company behind the open-source LangChain framework, and integrates seamlessly with it.\\n\\nKey Features of LangSmith\\n\\nDebugging and Testing: LangSmith isn't just about building; it's about building right. The platform offers interactive tutorials and a quick start guide to get you up and running. Whether you're coding in Python, TypeScript, or any other language, LangSmith has got you covered.\\n\\nAPI and Environment Setup: Before you start building, you'll need to set up your environment. LangSmith makes this easy with its API key access and straightforward environment configuration steps. For instance, you can install the latest version of LangChain for your target environment using simple commands like pip install -U langchain.\\n\\nTracing Capabilities: One of the standout features of LangSmith is its ability to trace code. This is crucial for debugging and improving your applications. You can customize run names, trace nested calls, and much more.\\n\\nWhy Choose LangSmith?\\n\\nEase of Use: LangSmith is designed with user-friendliness in mind. The platform offers a range of tutorials and documentation to help you get started.\\n\\nVersatility: Whether you're working on a small project or a large-scale application, LangSmith is versatile enough to meet your needs.\\n\\nCommunity Support: LangSmith has a strong community of developers and experts who are always ready to help. You can join the community forums or even contribute to the Cookbook with your own examples.\\n\\nBy now, you should have a good understanding of what LangSmith is and why it's a valuable asset for anyone working with LLMs. In the next section, we'll delve deeper into how to set up LangSmith and make the most of its features.\\n\\nSetting Up LangSmith\\n\\nSetting up LangSmith is a breeze, thanks to its user-friendly interface and well-documented steps. But before you dive in, you'll need an API key for access. Don't worry; getting one is as easy as pie.\\n\\nSteps to Get Your API Key\\n\\nCreate a LangSmith Account: Head over to the LangSmith website and sign up for an account. You can use various supported login methods.\\n\\nNavigate to Settings: Once your account is set up, go to the settings page. Here, you'll find the option to create an API key.\\n\\nGenerate API Key: Click on the 'Generate API Key' button, and voila! You have your API key.\\n\\nConfiguring Your Environment\\n\\nAfter obtaining your API key, the next step is to configure your runtime environment. LangSmith allows you to do this using simple shell commands. Here's how:\\n\\nexport LANGCHAIN_TRACING_V2=true export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com export LANGCHAIN_API_KEY=<your-api-key>\\n\\nReplace <your-api-key> with the API key you generated earlier. These commands set up your environment variables, making it easier to interact with LangSmith.\\n\\nLangSmith Cookbook: Real-world Lang Smith Examples\\n\\nThe LangSmith Cookbook is not just a compilation of code snippets; it's a goldmine of hands-on examples designed to inspire and assist you in your projects. Whether you're a beginner or an expert in the field of Language Learning Models (LLMs), the Cookbook offers a wealth of practical insights into common patterns and real-world use-cases. So, let's dig deeper into what the LangSmith Cookbook has to offer.\\n\\nWhat is the LangSmith Cookbook?\\n\\nThe LangSmith Cookbook is a repository that serves as your practical guide to mastering LangSmith. It goes beyond the basics covered in standard documentation, diving into common patterns and real-world scenarios. These recipes empower you to debug, evaluate, test, and continuously improve your LLM applications.\\n\\nThe Cookbook is a community-driven resource. If you have insights to share or feel that a specific use-case has been missed, you're encouraged to raise a GitHub issue or contact the LangChain development team. Your expertise shapes this community, making the Cookbook a dynamic and ever-evolving resource.\\n\\nKey Examples from the Cookbook\\n\\nTracing without LangChain (opens in a new tab): Learn how to trace applications independently of LangChain using Python SDK's @traceable decorator.\\n\\nREST API (opens in a new tab): Get acquainted with REST API features for logging LLM and chat model runs, and understand nested runs.\\n\\nCustomizing Run Names (opens in a new tab): Improve UI clarity by assigning specific names to LangSmith chain runs. This includes examples for chains, lambda functions, and agents.\\n\\nTracing Nested Calls within Tools (opens in a new tab): Learn how to include all nested tool subcalls in a single trace.\\n\\nDisplay Trace Links (opens in a new tab): Speed up your development by adding trace links to your application. This allows you to quickly see its execution flow, add feedback to a run, or add the run to a dataset.\\n\\nRetrievalQA Chain (opens in a new tab): Use prompts from the Hub in an example RAG pipeline.\\n\\nPrompt Versioning (opens in a new tab): Ensure deployment stability by selecting specific prompt versions.\\n\\nRunnable PromptTemplate (opens in a new tab): Save prompts to the Hub from the playground and integrate them into runnable chains.\\n\\nTesting & Evaluation\\n\\nQ&A System Correctness (opens in a new tab): Evaluate your retrieval-augmented Q&A pipeline end-to-end on a dataset.\\n\\nEvaluating Q&A Systems with Dynamic Data (opens in a new tab): Use evaluators that dereference labels to handle data that changes over time.\\n\\nRAG Evaluation using Fixed Sources (opens in a new tab): Evaluate the response component of a RAG pipeline by providing retrieved documents in the dataset.\\n\\nComparison Evals (opens in a new tab): Use labeled preference scoring to contrast system versions and determine the most optimal outputs.\\n\\nLangSmith in Pytest (opens in a new tab): Benchmark your chain in pytest and assert aggregate metrics meet the quality bar.\\n\\nUnit Testing with Pytest (opens in a new tab): Write individual unit tests and log assertions as feedback.\\n\\nEvaluating Existing Runs (opens in a new tab): Add AI-assisted feedback and evaluation metrics to existing run traces.\\n\\nNaming Test Projects (opens in a new tab): Manually name your tests with run_on_dataset(..., project_name='my-project-name').\\n\\nHow to Download Feedback and Examples (opens in a new tab): Export predictions, evaluation results, and other information to add to your reports programmatically.\\n\\nTypeScript / JavaScript Testing Examples\\n\\nEvaluating JS Chains in Python (opens in a new tab): Evaluate JS chains using custom Python evaluators.\\n\\nLogging Assertions as Feedback (opens in a new tab): Convert CI test assertions into LangSmith feedback.\\n\\nStreamlit Chat App (opens in a new tab): A minimal chat app that captures user feedback and shares traces of the chat application.\\n\\nNext.js Chat App (opens in a new tab): A Chat app but for Next.js version.\\n\\nReal-time Automated Feedback (opens in a new tab): Generate feedback metrics for every run using an async callback.\\n\\nReal-time RAG Chat Bot Evaluation (opens in a new tab): Automatically check for hallucinations in your RAG chat bot responses against the retrieved documents.\\n\\nExporting Data for Fine-tuning\\n\\nOpenAI Fine-Tuning (opens in a new tab): List LLM runs and convert them to OpenAI's fine-tuning format.\\n\\nLilac Dataset Curation (opens in a new tab): Further curate your LangSmith datasets using Lilac to detect near-duplicates and check for PII.\\n\\nExploratory Data Analysis\\n\\nExporting LLM Runs and Feedback (opens in a new tab): Extract and interpret LangSmith LLM run data for various analytical platforms.\\n\\nLilac (opens in a new tab): Enrich datasets using the open-source analytics tool, Lilac, to better label and organize your data.\\n\\nBy exploring these examples, you'll gain a comprehensive understanding of LangSmith's capabilities, empowering you to take your LLM applications to the next level. So why wait? Dive into the LangSmith Cookbook and start cooking up some code magic!\\n\\nLangSmith is not just another tool; it's a comprehensive platform that can take your LLM applications to the next level. From its robust tracing capabilities to its seamless integration with the LangChain Hub, LangSmith offers a range of features designed to make your life easier. And let's not forget the LangSmith Cookbook, a treasure trove of real-world examples and hands-on code snippets. Whether you're just starting out or looking to optimize your existing applications, LangSmith has got you covered.\\n\\nWhat does LangSmith do?\\n\\nLangSmith is a platform designed to help you build, test, evaluate, and monitor LLM applications. It offers a range of features including tracing, API access, and a Cookbook filled with real-world examples.\\n\\nWhat is the difference between LangSmith and LangChain?\\n\\nWhile LangSmith is focused on building and managing LLM applications, LangChain serves as a framework for developing language models. LangSmith integrates seamlessly with LangChain, offering a unified platform for all your LLM needs.\\n\\nHow do I get access to LangSmith?\\n\\nTo get access to LangSmith, you'll need to sign up for an account on their website. Once registered, you can generate an API key that will allow you to interact with the platform.\\n\\nLangServe: Tutorial for Easy LangChain Deployment[LangChain Tutorial] How to Add Memory to load_qa_chain and Answer Questions\", 'timestamp': '2024-02-26T20:12:29', 'title': 'LangSmith: Best Way to Test LLMs and AI Application – AI StartUps Product Information, Reviews, Latest Updates', 'url': 'https://cheatsheet.md/langchain-tutorials/langsmith'}, {'id': 'web-search_17', 'snippet': 'Enhancing Language Model Applications with LangChain: Practical Use Cases and Code Examples\\n\\nIntroduction to LangChain\\n\\nLangChain is a framework designed to amplify the capabilities of language models. It\\'s particularly adept at creating context-aware applications and enabling complex reasoning. This framework is comprised of LangChain Libraries, LangChain Templates, LangServe, and LangSmith, offering a comprehensive suite of tools for developers working with language models\\u200b\\u200b.\\n\\nCore Components of LangChain\\n\\nThese libraries provide modular, easy-to-use tools and integrations, simplifying the construction of complex chains for various tasks\\u200b\\u200b.\\n\\nLangChain Expression Language (LCEL)\\n\\nLCEL is a declarative language within LangChain, allowing for the seamless composition of chains from simple to complex tasks\\u200b\\u200b.\\n\\nLangChain in Action: Real-World Examples\\n\\n1. Document Question Answering\\n\\nLangChain can efficiently interface with a document database to extract and synthesize information in response to queries.\\n\\nfrom langchain.llms import OpenAI from langchain.chains import SimpleChain llm = OpenAI(api_key=\"YOUR_API_KEY\") chain = SimpleChain(llm=llm, components=[\"document_retrieval\", \"question_answering\"]) result = chain.run(\"What are the benefits of renewable energy?\", context_document=\"path/to/document.pdf\") print(result)\\n\\n2. Building Chatbots\\n\\nLangChain simplifies the creation of sophisticated chatbots that can handle complex interactions.\\n\\nfrom langchain.chains import ConversationalChain chatbot_chain = ConversationalChain() response = chatbot_chain.run(\"How can I improve my sleep quality?\") print(response)\\n\\n3. Analyzing Structured Data\\n\\nLangChain is also adept at processing and extracting insights from structured data, such as financial reports or customer databases.\\n\\nWritten by The Data Beast', 'timestamp': '2024-02-03T21:33:09', 'title': 'Enhancing Language Model Applications with LangChain: Practical Use Cases and Code Examples | by The Data Beast | Medium', 'url': 'https://medium.com/@thedatabeast/enhancing-language-model-applications-with-langchain-practical-use-cases-and-code-examples-c8527c9e8a58'}] is_search_required=None search_queries=[ChatSearchQuery(text='langserve langsmith', generation_id='ed0901c5-8c9e-4024-9b28-f28947677655'), ChatSearchQuery(text='langchain', generation_id='ed0901c5-8c9e-4024-9b28-f28947677655')] search_results=[ChatSearchResult(search_query=ChatSearchQuery(text='langchain', generation_id='ed0901c5-8c9e-4024-9b28-f28947677655'), connector=ChatSearchResultConnector(id='web-search'), document_ids=['web-search_0', 'web-search_1', 'web-search_2', 'web-search_3', 'web-search_4', 'web-search_5', 'web-search_9']), ChatSearchResult(search_query=ChatSearchQuery(text='langserve langsmith', generation_id='ed0901c5-8c9e-4024-9b28-f28947677655'), connector=ChatSearchResultConnector(id='web-search'), document_ids=['web-search_10', 'web-search_11', 'web-search_12', 'web-search_13', 'web-search_14', 'web-search_15', 'web-search_17', 'web-search_18'])] finish_reason='COMPLETE' tool_calls=None chat_history=[ChatMessage(role='USER', message='What is langchain?'), ChatMessage(role='CHATBOT', message='LangChain is a framework for developing applications powered by language models.'), ChatMessage(role='USER', message=\"What is langserve and langsmith and how it's different from langchain?\"), ChatMessage(role='CHATBOT', message='LangChain is an open-source framework for building applications using Large Language Models (LLMs). The framework simplifies the creation of applications by providing abstractions and tools to improve the customization, accuracy, and relevance of the information generated by the models. \\n\\nLangServe is a library for deploying LangChain applications as a REST API. It makes it easy to deploy any LangChain chain/agent/runnable and is designed for developers who leverage AI in their applications. LangServe provides features like automatic input and output schema inference, efficient API endpoints, and built-in monitoring through LangSmith.\\n\\nLangSmith is a platform for debugging, testing, evaluating, and monitoring LLM applications. It offers a unified developer platform for building, testing, and monitoring LLM applications built with or without LangChain. LangSmith helps in tracing and evaluating applications and provides a range of tutorials and real-world examples to assist developers.')] response_id='1710cf09-8a06-42d1-a27e-05c991689346' token_count={'prompt_tokens': 32922, 'search_query_tokens': 6, 'response_tokens': 189, 'total_tokens': 33111, 'billed_tokens': 32285} meta={'api_version': {'version': '1'}, 'billed_units': {'input_tokens': 32096, 'output_tokens': 189}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HgiTGHBG9Vs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}